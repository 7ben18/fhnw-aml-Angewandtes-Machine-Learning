---
title: "aml-mc-hs23-ben"
author: "Si Ben Tran"
date: "31.12.2023"
subtitle: Product Affinity Modelling
---

# Aufgaben
Aufgabe: Entwickle und evaluiere Affinitätsmodelle für Kreditkarten auf 
Basis von transaktionellen Kundeninformationen mittels binärer 
Klassifikation in Hinsicht auf personalisierte Werbekampagnen.

# Inhalt
• Aufbereitung eines Modellierungsdatensatz,  
• Modellentwicklung und systematischer Performance-Vergleich,  
• Vergleich der Haupteinflussfaktoren und Top-N Listen der Modelle,  
• Modell-Selektion und systematische Hyperparameter-Optimierung,  
• Modellvereinfachung und -beschreibung für Non-Data Scientist.  

# Libraries Laden
```{r echo=FALSE}
library(tidyverse)
library(tidymodels)
library(DataExplorer)
library(lubridate)
library(eeptools)
library(zoo)
library(arrow)
library(caret)
library(pROC)
library(ranger)
```


# 1. Daten
Laden, transformieren und überprüfen der Datenqualität mittels 
explorativer Datenanalyse; entfernen von Junior-Kreditkarten Kunden.

## 1.1 Laden
Insgesamt werden 8 verschiedene csv Files zur Verfuegung gestellt.
Die Beschreibungen der Daten sind hier zu finden: https://sorry.vse.cz/~berka/challenge/PAST/index.html

```{r read dataframe}
# alle file namen
file_names <- c(
  "account.csv", "card.csv", "client.csv", "disp.csv",
  "district.csv", "loan.csv", "order.csv", "trans.csv"
)

# leere liste fuer dataframes
data_frames <- list()

# For loop durch file_name
for (file_name in file_names) {
  # path definieren
  file_path <- file.path("xselling_banking_data", file_name)

  # daten einlesen und in liste speichern
  data_frames[[file_name]] <- read.csv(file_path, header = TRUE, sep = ";")
}

# dataframes zuweisen
account <- data_frames[["account.csv"]]
card <- data_frames[["card.csv"]]
client <- data_frames[["client.csv"]]
disp <- data_frames[["disp.csv"]]
district <- data_frames[["district.csv"]]
loan <- data_frames[["loan.csv"]]
order <- data_frames[["order.csv"]]
trans <- data_frames[["trans.csv"]]
```


## 1.2 transformieren
### 1.2.1 account
Die Tabelle account besitzt 4 Spalten, die account_id selbst, district_id welches auf die tabelle district verweist. 
Frequency welches die Haeufigkeit der Austellung von Kontoauszuegen definiert sowie ein datum. 

Die Werte von frequency und datum muessen umgeaendert werden, weiter aendern wir ebenfalls den spaltenname von frequency zu issuance_statement_frequency.
```{r wrangle account}
# Frequency
account <- account %>%
  mutate(frequency = case_when(
    frequency == "POPLATEK MESICNE" ~ "MONTHLY ISSUANCE",
    frequency == "POPLATEK TYDNE" ~ "WEEKLY ISSUANCE",
    frequency == "POPLATEK PO OBRATU" ~ " ISSUANCE AFTER TRANSACTION",
    TRUE ~ frequency # Keep other values unchanged
  )) %>%
  rename(issuance_statement_frequency = frequency)

# Datum
account$date <- ymd(account$date)

# Zuweisen zur df liste
data_frames[["account.csv"]] <- account

account %>% sample_n(size = 5)
```

### 1.2.2 card
Die Tabelle card beinhaltet die card_id, dis_id (verweist auf disp df) 
type der Karte und issued. 
Bei card aendern wir das Format in YYMMDD um und ignorierien die Zeit. 
(Auch gemaess der Dokumentation, ist keine Zeit vorgesehen.)
```{r wrangle card}
# Datum formatieren
card$issued <- as_date(ymd_hms(card$issued))

# Zuweisen zur df Liste
data_frames[["card.csv"]] <- card

# Df ausgeben
card %>% sample_n(size = 5)
```

### 1.2.3 client
client Tabelle hat client_id, birht_number mit einem speziellen Format, YYMMDD
fuer Maenner und YYMM + 50 DD fuer Frauen. Also sprich, wurden bei Frauen der Geburtsmonat um 50 addiert. Weiter gibt es noch die district_id, welches auf die district Tabelle verweist. 
```{r wrangle client}
# Funktion zur Geschlechterbestimmung
get_gender <- function(birth_number) {
  year <- as.integer(substr(birth_number, 1, 2))
  month <- as.integer(substr(birth_number, 3, 4))
  day <- as.integer(substr(birth_number, 5, 6))

  # Bei Frauen wird der Geburtsmonat um 50 addiert
  if (month > 12) {
    gender <- "female"
  } else {
    gender <- "male"
  }

  return(gender)
}

# Geschlecht bestimmen und als neue Spalte hinzufügen
client$gender <- sapply(client$birth_number, get_gender)


# filter nach Frauen, erstelle jahr (19XX), Monat - 50 und Tag Spalte sowie Geburtstag.
client_female <- client %>%
  filter(gender == "female") %>%
  mutate(birth_number,
    year =
      as.integer(paste("19", as.character(substr(birth_number, 1, 2)), sep = "")),
    month =
      as.integer(substr(birth_number, 3, 4)) - 50,
    day =
      as.integer(substr(birth_number, 5, 6)),
    birth_day =
      make_date(year, month, day)
  ) %>%
  select(client_id, year, month, day, birth_number = birth_day, district_id, gender)

# Analog fuer Maenner
client_male <- client %>%
  filter(gender == "male") %>%
  mutate(birth_number,
    year =
      as.integer(paste("19", as.character(substr(birth_number, 1, 2)), sep = "")),
    month =
      as.integer(substr(birth_number, 3, 4)),
    day =
      as.integer(substr(birth_number, 5, 6)),
    birth_day =
      make_date(year, month, day)
  ) %>%
  select(client_id, year, month, day, birth_number = birth_day, district_id, gender)

# Anzahl Zeilen ueberpruefen, ob diese mit dem Ursprungs DataFrame uebereinstimmen.
if (nrow(rbind(client_female, client_male)) == nrow(client)) {
  client <- rbind(client_female, client_male)
}

# Berechnung des Alters auf basisjahr 31.12.1999
client$age <- age_calc(dob = client$birth_number, enddate = as.Date("1999-12-31"), units = "years", precise = FALSE)

# zuweisen zur df liste
data_frames[["client.csv"]] <- client

# ausgeben
client %>% sample_n(size = 5)
```

### 1.2.4 disp
disp hat disp_id, client_id, welches auf client dataframe verweist und account_id, welches auf account Tabelle verweist sowie den typ, wobei nur OWNER einen Dauerauftrag erstellen koennen und Kredite aufnehmen koennen.
```{r wrangle disp}
disp %>% sample_n(size = 5)
```

### 1.2.5 district
district hat nummerierte Spalten von A1 - A16. Dabei sind die Kurzel jeweils auf der Website aufgelistet fuer was diese stehen. Wir bennen die Spalten um, damit wir eine Ahnung haben, mit was wir arbeiten sollten. 

A1  district_id/district code	
A2	district name	
A3	region	
A4	no. of inhabitants	
A5	no. of municipalities with inhabitants < 499	
A6	no. of municipalities with inhabitants 500-1999	
A7	no. of municipalities with inhabitants 2000-9999	
A8	no. of municipalities with inhabitants >10000	
A9	no. of cities	
A10	ratio of urban inhabitants	
A11	average salary	
A12	unemploymant rate '95	
A13	unemploymant rate '96	
A14	no. of enterpreneurs per 1000 inhabitants	
A15	no. of commited crimes '95	
A16	no. of commited crimes '96

```{r wrangle district}
district <- district %>% select(
  district_id = A1, district_name = A2,
  region = A3, num_of_habitat = A4,
  num_of_small_town = A5, num_of_medium_town = A6,
  num_of_big_town = A7, num_of_bigger_town = A8,
  num_of_city = A9, ratio_of_urban = A10,
  average_salary = A11, unemploy_rate95 = A12,
  unemploy_rate96 = A13, n_of_enterpren_per1000_inhabit = A14,
  no_of_crimes95 = A15, no_of_crimes96 = A16
)

# zuweisen zur df liste
data_frames[["district.csv"]] <- district

district %>% sample_n(size = 5)
```

### 1.2.6 loan
Loan hat loan_id, account_id (verweis auf account Tabelle), date in YYMMDD Format, amount, duration, payments, status.
Aendern werden wir payments zu monthly_payments und die Werte von
status. 
'A' stands for contract finished, no problems, => contract finished
'B' stands for contract finished, loan not payed, => finished contract, loan not payed
'C' stands for running contract, OK so far, => running contract
'D' stands for running contract, client in debt => client in debt

Weiter checken wir, ob ein Account mehrere Loans haben kann, Gruppieren nach account und zaehlen die Anzahl Zeilen. Diese Fuegen wir dann spaeter zu unserem Loan DataFrame hinzu als weiteres Feature. 

```{r wrangle loan}
loan$date <- ymd(loan$date)

loan <- loan %>%
  mutate(status = case_when(
    status == "A" ~ "contract finished",
    status == "B" ~ "finished contract, loan not paid",
    status == "C" ~ "running contract",
    status == "D" ~ "client in debt",
  ))

num_of_loan_df <- loan %>%
  group_by(account_id) %>%
  summarize(num_of_loan = n()) %>%
  arrange(desc(num_of_loan))

num_of_loan_df

loan <- inner_join(
  x = loan,
  y = num_of_loan_df,
  by = "account_id"
)


# Zuweisen zur df liste
data_frames[["loan.csv"]] <- loan

loan %>% sample_n(size = 5)
```

### 1.2.7 order
die order Tabelle besteht aus order_id, account_id, bank_to (einzigartige Komibination aus 2 Buchstaben), account_to (zu welchem Account), amount, k_symbol (klassifizierung der Zahlung)
k_symbol wird auf englisch uebersetzt

Da es accounts gibt die keine Orders haben, versuchen wir den Order Datensatz zu erweitern, indem wir von account, account_id einen left join machen. Somit erhalten wir alle account auch diejenigen die keine order besitzen. Diese haben NA Werte, welche wir dann mit 0 beim Amount und "UNKOWN" bei k_symbol ersetzen.

Ein account kann mehrere Orders haben, um spaeter einfacher joinen zu koenen versuchen wir durch aggregieren nach account_id die Summe, Mittelwert, Median Ausgaben zu erhalten. Bei den Kategorischen Spalte k_symbol versuchen wir diese One-Hot zu encoden. 
```{r wrangle order}
order <- order %>%
  mutate(k_symbol = case_when(
    k_symbol == "POJISTNE" ~ "INSURRANCE PAYMENT",
    k_symbol == "SIPO" ~ "HOUSHOLD",
    k_symbol == "UVER" ~ "LOAN PAYMENT"
  ))

order$k_symbol[is.na(order$k_symbol)] <- "UNKOWN"


# create a dataframe that contains all account_id from account
account_id_df <- account %>%
  select(account_id)

# make a left join with order, every account that doesnt have an order will have NA values
order <- left_join(
  x = account_id_df,
  y = order,
  by = "account_id"
) %>% # create a column has_order if the row is complete
  mutate(has_order = !rowSums(is.na(.))) # if k_symbol NA then replace with "UNKOWN"

order$k_symbol[is.na(order$k_symbol)] <- "UNKOWN"

order$amount[is.na(order$amount)] <- 0

# Aggregating amount
aggregated_amount <- order %>%
  group_by(account_id) %>%
  summarise(
    sum_amount = sum(amount, na.rm = TRUE),
    mean_amount = mean(amount, na.rm = TRUE),
    median_amount = median(amount, na.rm = TRUE),
    min_amount = min(amount, na.rm = TRUE),
    max_amount = max(amount, na.rm = TRUE),
    num_of_orders = sum(amount != 0) # Count non-zero orders
  )


# create a column has_order if the sum_amount is 0 == False
aggregated_amount$has_order <- aggregated_amount$sum_amount != 0

aggregated_amount

# Creating dummies
dummies_k_symbol <- order %>%
  select(account_id, k_symbol) %>%
  pivot_wider(
    names_from = k_symbol,
    values_from = k_symbol,
    values_fill = 0,
    values_fn = length
  )

dummies_k_symbol

# Merging dataframes
order <- left_join(aggregated_amount, dummies_k_symbol, by = "account_id")

# zuweisen zur df liste
data_frames[["order.csv"]] <- order

order %>% sample_n(size = 5)
```


### 1.2.8 trans
Die trans Tabelle hat trans_id, account_id, date, type, operation, amount und banalce, k_symbol (analog zu order) bank und account


type, operation und k_symbol Werte werden auf Englisch uebersetzt
```{r wrangle trans}
trans <- trans %>%
  mutate(
    date = ymd(date),
    type = case_when(
      type == "PRIJEM" ~ "CREDIT",
      type == "VYDAJ" ~ "WITHDRAWAL"
    ),
    operation = case_when(
      operation == "VYBER KARTOU" ~ "CREDIT CARD WITHDRAWAL",
      operation == "VKLAD" ~ "CREDIT IN CASH",
      operation == "PREVOD Z UCTU" ~ "COLLECTION FROM ANOTHER BANK",
      operation == "VYBER" ~ "WITHDRAWAL IN CASH",
      operation == "PREVOD NA UCET" ~ "REMITTANCE TO ANOTHER BANK"
    ),
    k_symbol = case_when(
      k_symbol == "POJISTNE" ~ "INSURRANCE PAYMENT",
      k_symbol == "SLUZBY" ~ "STATEMENT PAYMENT",
      k_symbol == "UROK" ~ "INTEREST CREDITED",
      k_symbol == "SANKC. UROK" ~ "INTERES IF NEGATIVE BALANCE",
      k_symbol == "SIPO" ~ "HOUSHOLD",
      k_symbol == "DUCHOD" ~ "OLD-AGE PENSION",
      k_symbol == "UVER" ~ "LOAN PAYMENT",
    )
  )

# Zuweisen zur df liste
data_frames[["trans.csv"]] <- trans

trans %>% sample_n(size = 5)
```

## 1.3 Datenqualität mittels eda

### 1.3.1 Fehlende Werte
Wir erkennen, dass die Transaktions Daten in der Tabelle order und trans fehlende Werte aufweisen. 

```{r fehlende Werte}
# check for missing values in each dataframe
for (i in 1:length(data_frames)) {
  df_name <- names(data_frames)[i] # Get the dataframe name
  cat("Missing values in", df_name, ":\n")
  print(sum(is.na(data_frames[[i]])))
}
```

### 1.3.1 District - Fehlende Werte

Bei genauer Betrachtung des Dataframes "district" ist aufgefallen, dass bei dessen Einlesen die Spalten "unemploy_rate95" und "no_of_crime95" als Zeichenketten (Character) eingelesen wurden. Bei beiden Spalten wurde der ein "?" verwendet statt NA. 
Hier in diesem Code Abschnitt imputieren den fehlenden Wert mit dem Mittelwert der jeweiligen Spalte.

```{r wramgle missing values, warning=FALSE}
# Datentyp convertieren
district$unemploy_rate95 <- as.numeric(district$unemploy_rate95)
district$no_of_crimes95 <- as.numeric(district$no_of_crimes95)

# fehlende Werte plotten
plot_missing(district)

# Imputation mittels mean
district$unemploy_rate95[is.na(district$unemploy_rate95)] <- median(district$unemploy_rate95, na.rm = TRUE)

district$no_of_crimes95[is.na(district$no_of_crimes95)] <- median(district$no_of_crimes95, na.rm = TRUE)

# fehlende Werte nach imputation plotten
plot_missing(district)
```


### 1.3.2 Data Explorer

Wir nutzen Data Explorer für eine erste Grobe Übersicht zu unseren Daten zu erhalten. 

```{r perform eda with DataExplorer}
# Function to perform EDA on a dataframe
perform_eda <- function(df, df_name) {
  # Create a data exploration report for the dataframe
  title <- paste("Exploratory Data Analysis for", df_name)
  create_report(df, report_title = title)

  # You can add more EDA steps here, such as additional plots and analysis
}

# Specify your directory path
path <- "eda-dataexplorer-reports"

# Check if directory exists
if (dir.exists(path)) {
  # List all files in directory
  list_of_files <- list.files(path)

  # Check if directory is empty
  if (length(list_of_files) == 0) {
    # Iterate through the list of dataframes and perform EDA on each
    for (i in 1:length(data_frames)) {
      df_name <- names(data_frames)[i] # Get the dataframe name
      perform_eda(data_frames[[i]], df_name)
    }
  } else {
    cat("Directory is not empty. Not running the EDA code.")
  }
} else {
  cat("Specified path does not exist. Please check the path and try again.")
}
```

Mittels der Funktion perform_eda() generiert für jedes Dataframe eine Explorative Datenanalyse in html Format. Diese sind unter dem Verzeichnis: eda-dataexplorer-reports zu finden. 
Wir betrachten nun alle Dateframes vom Dataexplorer und halten unsere Kentnisse in diesem Abschnitt fest. 

account: 
Account besitzt keine fehlende Werte. Weiter fällt stark auf, dass bei issuance_statement_frequency der meiste vorkommende Wert MONTHLY ISSUANCE ist. 

card:
card besitzt keine fehlende Werte. Beim Barplot vom Typ sehen wir, dass classic die am häufigsten vertretene Klasse ist. Gefolgt wird diese von Junior und auf dem dritten Platz, mit am wenigsten vorkommenden sind gold.

client:
client weist keine fehlende Werte im Datensatz auf. Die Altersverteilung ist leicht linksschief. Wir erkennen im korrelationsplot, dass Jahr und Alter korrelieren, was jedoch nicht verwunderlich ist.

disp:
disp weist keine fehlende Werte im Datensatz auf. Die am meisten vorkommende Variable von type ist OWNER. DISPONENT kommt 1/4 mal weniger vor als OWNER.

district:
district weist ebenfalls keine fehlende Werte auf. average_salary ist rechtsschief verteilt. 

loan:

order:

trans:

## 1.4 Junioren entfernen

### 1.4.1 entfernen von Junior Kreditkarten Kunden
```{r remove junior card}
# checken, welche Variabeln vorhanden sind.
print(paste("Kartentypen:", unique(card$type)))

# !junior dataframe
card_clean <- subset(card, type != "junior")

# junior dataframe
card_junior <- subset(card, type == "junior")

# Berechne Differenz von rows zwischen card_junior und card
print(paste("Anzahl entfernte junior Karten:", nrow(card_junior)))

# entfernen von allen disp_id in disp Dataframe, die in card_junior vorkommen.  (1:1)
disp_clean <- subset(disp, !(disp_id %in% unique(card_junior$disp_id)))
print(paste("Anzahl entfernte disp_id, die von card_junior kommen:", nrow(disp) - nrow(disp_clean)))
```

### 1.4.2 entfernen von junge Kunden

Wir untersuchen die Junioren genauer indem wir eine Visualisierung erstellen, indem wir die Altersverteilungen der Kunden sehen aufgrund dessen, welche Karten sie besitzen. 
Dazu muessen wir schon einige Tabellen miteinander joinen um dies tun zu koennen. 
Weiter haben wir eine Statistische Auswertung des Alters gemacht, indem wir nach card.type gruppieren und das jeweilige Alter aggregieren. Es stellt sich heraus, das die aelteste Person 25 Jahre Alt ist und eine junior Karte bestizt. Da wir in unserem Modell nicht die Junioren ansprechen wollen, entfernen wir zusaetzlich noch alle Kunden die juenger als 26 sind. Relevant bleiben uns somit nur die accounts_id die ueber 25 Jahre alt sind. 

```{r}
# inner join by client_id
client_disp <- inner_join(
  x = client,
  y = disp,
  by = "client_id",
  suffix = c(".client", ".disp")
) %>%
  select(
    client_id,
    district_id,
    disp_id,
    account_id,
    everything()
  ) %>%
  rename(
    "year.client" = "year",
    "month.client" = "month",
    "day.client" = "day",
    "birht_number.client" = "birth_number",
    "gender.client" = "gender",
    "age.client" = "age",
    "type.disp" = "type"
  )

# left join
client_disp_card <- left_join(
  x = client_disp,
  y = card,
  by = "disp_id",
  suffix = c("", ".card")
) %>%
  mutate(has_card = !rowSums(is.na(.))) %>%
  select(-c(disp_id, card_id)) %>%
  rename(
    "type.card" = "type",
    "issued.card" = "issued",
    "has_card.card" = "has_card"
  )

client_disp_card

# create age distribution and color them with type.card
ggplot(client_disp_card, aes(
  x = age.client,
  color = type.card
)) +
  geom_histogram(
    alpha = 0.5,
    fill = "white"
  ) +
  labs(title = "Verteilung von Alter unterteilt nach Kartentyp", x = "Alter", y = "Anzahl") +
  facet_wrap(~type.card)

# get statistical information grouped by type.card for age
client_disp_card %>%
  group_by(type.card) %>%
  summarize(
    mean_age = mean(age.client),
    median_age = median(age.client),
    min_age = min(age.client),
    max_age = max(age.client),
    quantile_75 = quantile(age.client, 0.75)
  )

# create boxplot for age distribution, for each card.type
ggplot(client_disp_card, aes(
  x = type.card,
  y = age.client,
  color = type.card
)) +
  geom_boxplot() +
  labs(title = "Verteilung von Alter unterteilt nach Kartentyp", x = "Kartentyp", y = "Alter")

# remove all junior in type.card but keep NA
client_disp_card_rm_junior <- client_disp_card %>%
  filter(type.card != "junior" | is.na(type.card))

# get all account_ids that are older than 25 years old
client_ids_older_than_25 <- client_disp_card_rm_junior %>%
  filter(age.client > 25) %>%
  select(client_id)

client_ids_older_than_25
```



# 2. Kombinieren 
Kombinieren der Informationen zu Kunden und Bankdienstleistungen.

Durch unsere Skizze die unter als "skizze-tabelle-joinen" abgelegt wurde, wissen wir nun wie die Tabellen miteinander verknuepft sind und konnen so einen Datensatz von Kunden und einen Datensatz von Bankdienstleistungen erstellen.  

## 2.1 client - disp

Wir innen joinen client und disp mit client_id, renamen die Spaltenamen und fügen jeweils ein suffix hinzu, um später noch eine Übersicht zu haben, von welcher Tabelle die Spalte entspringt. Beim inner joinen verlieren wir keine Kunden. Nach dem Joinen wissen wir von jedem Kunden, was für eine Art type der Kunde ist. OWNER oder DISPONENT.  

```{r cmobine client-disp}
# client und disp weisen eine 1:1 relation auf client_id auf.
print(paste("Anzahl Zeilen client_ids_older_than_25 df:", nrow(client_ids_older_than_25)))
print(paste("Anzahl Zeilen disp df:", nrow(disp)))

# Inner join client with client_ids_older_than_25 to remove junior and clients age <= 25
client <- inner_join(
  x = client,
  y = client_ids_older_than_25,
  by = "client_id"
)

# inner join by client_id
client_disp <- inner_join(
  x = client,
  y = disp,
  by = "client_id",
  suffix = c(".client", ".disp")
) %>%
  select(
    client_id,
    district_id,
    disp_id,
    account_id,
    everything()
  ) %>%
  rename(
    "year.client" = "year",
    "month.client" = "month",
    "day.client" = "day",
    "birht_number.client" = "birth_number",
    "gender.client" = "gender",
    "age.client" = "age",
    "type.disp" = "type"
  )

print(paste("Anzahl Zeilen client_disp df:", nrow(client_disp)))
client_disp
```

### 2.1.1 gender vs. type
Die meisten unserer Kunden sind vom typ her OWNER. Das Geschlecht ob man OWNER oder DISPONENT ist sehen bei beiden Verteilungen ähnlich gleich aus. 

```{r eda}
ggplot(client_disp, aes(
  x = age.client,
  color = gender.client
)) +
  geom_histogram(
    alpha = 0.2,
    fill = "white",
    position = "dodge"
  ) +
  labs(
    title = "Verteilung von Alter unterteilt nach Geschlecht und Typ",
    x = "Alter",
    y = "Anzahl"
  ) +
  facet_grid(gender.client ~ type.disp)
```

## 2.2 +card

Wir joinen client_disp mit card über disp_id. Nicht jeder Kunde besitzt eine Karte, somit ist ein left join auf client_disp unsere Wahl, um keine Kunden zu verlieren. Weiter wurde eine weitere spalte erstellt, die uns markiert, ob der Kunde eine Karte besitzt oder nicht. Dies könnte später noch relevant werden bei der Evaluierung unsere Modelle.
```{r +card}
# Weiter joinen wir client_disp und card mit disp_id. Auch hier besteht eine 1:1 Beziehung.

# Wir erkennen jedoch, dass es clienten gibt, die keine Karte besitzen. aus diesem Grund

print(paste("Anzahl Zeilen card df:", nrow(card_clean)))

# left join
client_disp_card <- left_join(
  x = client_disp,
  y = card_clean,
  by = "disp_id",
  suffix = c("", ".card")
) %>%
  mutate(has_card = !rowSums(is.na(.))) %>%
  select(-c(client_id, disp_id, card_id)) %>%
  rename(
    "type.card" = "type",
    "issued.card" = "issued",
    "has_card.card" = "has_card"
  )

print(paste("Anzahl Zeilen client_disp_card df:", nrow(client_disp_card)))
client_disp_card
```
### 2.2.1 type vs. has_card
Aus der Visualisierung erkennen wir, dass keine Disponenten existieren die eine Karte besitzen. Aus diesem Grund filtern wir unseren Datensatz nur nach Owner, da wir diesen Personen eine Karte anbieten, bzw Karten wie classic oder gold besitzen bzw. verkaufen koennten. 

```{r eda}
ggplot(client_disp_card, aes(
  x = age.client,
  color = has_card.card
)) +
  geom_histogram(
    alpha = 0.5,
    fill = "white"
  ) +
  labs(title = "Verteilung von Alter unterteilt nach Kartentyp und Art", x = "Alter", y = "Anzahl") +
  facet_grid(type.card ~ type.disp)
```

### 2.2.2 groupby account_id
Kann ein Kunde Disponent und OWNER gleichzeitig sein?

Ja es gibt Account die mehrere Zeilen beinhalten. 
```{r eda}
# groupby account and count number of rows
client_disp_card %>%
  group_by(account_id) %>%
  summarize(num_of_rows = n()) %>%
  arrange(desc(num_of_rows))
```

## 2.3 Filter nach OWNER

Durch das Filtern verlieren wir 869 Kunden. Es bleiben von den 5369 Kunden nun noch 4500. 
```{r filter for owner}
# Filtern nach type.disp == "OWNER"
print(paste("Anzahl entfernte Zeilen von client_disp_card df:", nrow(client_disp_card) - nrow(client_disp_card %>% filter(type.disp == "OWNER"))))

client_disp_card <- client_disp_card %>% filter(type.disp == "OWNER")
client_disp_card
```

### 2.3.1 groupby account_id

Das das Filtern stellen wir nun fest, dass jeder Kunde nur ein Account bei der Bank besitzt. Diese koennen wir nun nutzen, um mit dem DataFrame account zu joinen. 
```{r eda}
# groupby account and count number of rows
client_disp_card %>%
  group_by(account_id) %>%
  summarize(num_of_rows = n()) %>%
  arrange(desc(num_of_rows))
```


## 2.4 + account
Durch die Erkentnisse von 2.3.1 koennen wir nun einen Inner Join durchfuehren von unserem vorhanden dataframe client-disp-card mit account Tabelle. 
Die Anzahl der Kunden bleibt somit gleich auf 4500. 
```{r +account}
# Ein Account kann mehrere disponenten haben.

print(paste("Anzahl Zeilen account df:", nrow(account)))
print(paste("Anzahl Zeilen client_disp_card:", nrow(client_disp_card)))


client_disp_card_account <- inner_join(
  x = account,
  y = client_disp_card,
  by = "account_id"
) %>%
  rename(
    "district_id.account" = "district_id.x",
    "district_id.client" = "district_id.y",
    "issuance_statement_frequency.account" = "issuance_statement_frequency",
    "account_creation_date.account" = "date"
  )


print(paste("Anzahl Zeilen client_disp_card_account df:", nrow(client_disp_card_account)))
client_disp_card_account
```

## 2.5 loan - account

Wir betrachten hier nun account und loan getrennt von unserem gejointen Dataframe. Erst in einem zweiten Schritt joinen wir beide Dataframes dann miteinander. Da ein Account ein Kredit haben kann oder nicht, fuehren wir einen left_join auf account ueber account_id.

```{r loan-account}
# Ein account kann ein Kredit haben oder auch nicht.
account_loan <- left_join(
  x = account,
  y = loan,
  by = "account_id"
) %>%
  mutate(has_loan.loan = !rowSums(is.na(.))) %>%
  rename(
    "date.loan" = "date.y",
    "amount.loan" = "amount",
    "duration.loan" = "duration",
    "payments.loan" = "payments",
    "status.loan" = "status",
    "num_of_loan.loan" = "num_of_loan"
  ) %>%
  select(-c(loan_id, district_id, issuance_statement_frequency, date.x))

account_loan
```

### 2.5.1 NA Werte
Durch das Joinen haben wir viele Spalten erhalten die NA Werte aufweisen. Bzw. nur die Spalten die von Loan kommen. Diese werden durch 0 oder no_loan ersetzt, da diese Accounts keinen Kredit besitzen. Beim Datum belassen wir die NA Werte

```{r reaplce NA values}
# reaplce NA values with 0 or no_loan
account_loan <- account_loan %>%
  mutate(
    amount.loan = ifelse(is.na(amount.loan), 0, amount.loan),
    duration.loan = ifelse(is.na(duration.loan), 0, duration.loan),
    payments.loan = ifelse(is.na(payments.loan), 0, payments.loan),
    status.loan = ifelse(is.na(status.loan), "no_loan", status.loan),
    num_of_loan.loan = ifelse(is.na(num_of_loan.loan), 0, num_of_loan.loan)
  )
```


### 2.5.2 amount vs. payments

Wir betrachten hier nun account und loan getrennt von unserem gejointen Dataframe. Erst in einem zweiten Schritt joinen account_loan und client_disp_card_account mit der Spalte account_id zusammen und fuehren eine kurze explorative Datenanalyse durch. 

Interessant zu sehen ist, dass die meisten Kredite schon abbezahlt wurden die eine Dauer von 12 Monaten aufweisen. 

Viele Account haben keinen Kredit aufgenommen.

```{r eda, warning=FALSE}
# Kredithoehe vs. Payments mit dauer und status
sample_n(account_loan, size = 1000) %>%
  ggplot(aes(
    x = amount.loan,
    y = payments.loan,
    shape = as.factor(duration.loan),
    color = status.loan
  )) +
  geom_point(alpha = 1) +
  labs(
    title = "Zahlung vs. Menge Kredit, unterteilt nach Dauer und",
    x = "Kreditmenge",
    y = "Zahlung"
  )

# Anzahl Kredite unterteilt nach Status
account_loan %>%
  ggplot(aes(
    x = has_loan.loan,
    fill = status.loan
  )) +
  geom_bar(position = "stack") +
  labs(
    title = "Stacked Barplot",
    x = "Kredit",
    y = "Anzahl"
  )
```


## 2.6 + account_loan

Wir joinen nun beide groesseren Dataframes ueber account_id. Beide Dataframes weisen 4500 Zeilen, somit haben wir nach dem joinen immer noch 4500 Kunden, welche 22 Features aufweisen. 
```{r +amount_loan}
# account_loan mit client_disp_card_district_account joinen
client_disp_card_account_loan <- inner_join(
  x = account_loan,
  y = client_disp_card_account,
  by = "account_id"
)

print(paste("Anzahl Zeilen client_disp_card_account df:", nrow(client_disp_card_account_loan)))
client_disp_card_account_loan
```



### 2.6.1 loan vs. card?
Was uns nun intressiert ist der Zusammenhang zwischen eine Karte zu besitzen und einen Kredit aufzuweisen. 

Es gibt Kunden die eine Karte haen und einen Kredit, analog gibt es auch Kunden die keine Karte haben, aber einen Kredit oder nicht. Somit behalten wir in unserem Datensatz die Kundschaft bei.

```{r eda}
client_disp_card_account_loan %>%
  ggplot(aes(
    x = has_loan.loan,
    fill = has_card.card
  )) +
  geom_bar(position = "dodge") +
  labs(
    title = "Barplot",
    x = "Kredit vorhanden?",
    y = "Anzahl"
  ) +
  facet_grid(type.card ~ issuance_statement_frequency.account)
```

### 2.6.2 Anzahl Kunden?
Wir überprüfen unser DataFrame ob eine Zeile einen Kunden entspricht indem wir nach account_id gruppieren und die Rows zaehlen.

In unserem DataFrame ist jede Zeile ein Kunde. Somit konnte jedem Kunden ein Bank Account zugewiesen werden. 
```{r eda}
# groupby account and count number of rows
client_disp_card_account_loan %>%
  group_by(account_id) %>%
  summarize(num_of_rows = n()) %>%
  arrange(desc(num_of_rows))
```

## 2.7 +order

Durch den Inner join mit order haben wir weiterhin die gleiche Anzahl Kunden.


```{r +order}
# inner join by account_id client_disp_card_district_account_loan und order

print(paste("Anzahl Zeilen client_disp_card_account_loan_order df:", nrow(client_disp_card_account_loan)))
print(paste("Anzahl Zeilen order :", nrow(order)))


client_disp_card_account_loan_order <- inner_join(
  x = client_disp_card_account_loan,
  y = order,
  by = "account_id"
) %>%
  rename(
    "sum_amount.order" = "sum_amount",
    "mean_amount.order" = "mean_amount",
    "median_amount.order" = "median_amount",
    "min_amount.order" = "min_amount",
    "max_amount.order" = "max_amount",
    "num_of_orders.order" = "num_of_orders",
    "num_insurrance_payment.order" = "INSURRANCE PAYMENT",
    "num_household.order" = "HOUSHOLD",
    "num_loan_payment.order" = "LOAN PAYMENT",
    "num_unkown.order" = "UNKOWN",
    "has_order.order" = "has_order"
  ) %>%
  arrange(account_id)

print(paste("Anzahl entfernte Zeilen von client_disp_card_account_loan df:", nrow(client_disp_card_account_loan) - nrow(client_disp_card_account_loan_order)))

client_disp_card_account_loan_order
```

## 2.8 +district
Wir erweitern unseren Datensatz mit dem district. Wichtig dabei zu beachten ist, dass wir jeweils einen district fuer die Filiale der Bank haben und einen district fuer den Kunden. In unserem Fall joinen wir unser Dataframe mit dem des Kunden, da uns dieser mehr interessiert, ob geographische Einfluesse den Kauf eines Bank Produktes.

```{r +district}
print(paste("Anzahl Zeilen district df:", nrow(district)))
print(paste("Anzahl Zeilen client_disp_card_account_loan_order:", nrow(client_disp_card_account_loan_order)))

client_disp_card_account_loan_order_district <- left_join(client_disp_card_account_loan_order %>% rename("district_id" = "district_id.client"),
  district,
  by = "district_id"
) %>%
  rename(
    "district_name.district" = "district_name",
    "region.district" = "region",
    "num_of_habitat.district" = "num_of_habitat",
    "num_of_medium_town.district" = "num_of_medium_town",
    "num_of_small_town.district" = "num_of_small_town",
    "num_of_big_town.district" = "num_of_big_town",
    "num_of_bigger_town.district" = "num_of_bigger_town",
    "num_of_city.district" = "num_of_city",
    "ratio_of_urban.district" = "ratio_of_urban",
    "average_salary.district" = "average_salary",
    "unemploy_rate95.district" = "unemploy_rate95",
    "unemploy_rate96.district" = "unemploy_rate96",
    "n_of_enterpren_per1000_inhabit.district" = "n_of_enterpren_per1000_inhabit",
    "no_of_crimes95.district" = "no_of_crimes95",
    "no_of_crimes96.district" = "no_of_crimes96"
  ) %>%
  select(-c(district_id))

print(paste("Anzahl Zeilen client_disp_card_account_loan_order_district df:", nrow(client_disp_card_account_loan_order_district)))

data <- client_disp_card_account_loan_order_district

data
```

## 2.9 Data
```{r}
data
```


## 2.10 trans
Ein Account kann mehrere Transaktionen gemacht haben. 
- droppen von unnoetigen Transaktion spalten
- Aggregation fuer Numerische (Mean, Median etc...)
- Aggregation mir Rolling Window von balance und
- OneHotEncoding fuer Kategorische

```{r +trans?}
trans <- trans %>%
  select(-c(trans_id, account)) %>%
  arrange(account_id)
trans
```

# 3. Identifizierung
Identifizieren bestehender Kreditkartenkäufer inkl. Bestimmung des Kaufdatums und Rollup-Fensters, definiert durch 1 Monat Lag und 12 Monate History vor Kreditkartenkauf.

## 3.1 Identifizierung bestehender Kreditkartenkaeufer und Kaufdatum

selektieren vom grossen dataframe has_card nach True und selektieren account_id und issued.card

```{r}
# create a dataframe where has_card.card == TRUE
account_ids_has_card <- data %>%
  filter(has_card.card == TRUE) %>%
  select(account_id, issued.card)

account_ids_has_card
```

## 3.2 Rollup-Fenster von Käufer

Das Rollup Fenster definieren wir indem wir vom Kaufdatum 13 Monate subtrahieren (12 Monate History + 1 Monat Lag). Vor dem Kauf einer Kreditkarte kann es gut sein, dass der Kunde sich 1 Monat schon davor entschieden hat eine Karte zu beantragen und diese noch von der Bank verarbeitet werden muss. Aus diesem Grund filtern wir die Transaktionellen Daten zwischen den Bereich start_date (kaufdatum.datum - 13 Monate) und start_lag_date (kaufdatum.card - 1 Monat). Somit erhalten wir unsere Transkationsdaten fuer Kunden die eine Karte haben im gegebenen Zeitfenster.


```{r}
# vom Kaufdatum -13 Monate von Kunden die eine Karte besitzen
account_ids_has_card <- account_ids_has_card %>%
  mutate(
    start_date = issued.card %m-% months(13),
    start_lag_date = issued.card %m-% months(1)
  )

account_ids_has_card

# get from transaction all accounts that got a card and filter also by its range
buyers <- inner_join(
  x = account_ids_has_card,
  y = trans,
  by = "account_id"
) %>%
  filter(date >= start_date & date <= start_lag_date) %>%
  select(-c(start_date, start_lag_date))

# extract year and month
buyers$year_month <- format(as.Date(buyers$date), "%Y-%m")

buyers

# reorder col
buyers <- buyers %>%
  select(account_id, issued.card, date, year_month, type:bank)

# check transaction within range
buyers
```

## 3.3 Roll-up Fenster Pivotieren
Ein Kunde der eine Karte hat, kann schon mehrere Transaktionen in einem Monat haben. Damit wir diese spaeter mit den non-buyers vergleichen koennen, extrahieren wir das Jahr und den Monat, gruppieren und pivotieren das Dataframe. So erhaltlen wir auf einer Zeile einen Kunden und deren zugehorigen Jahr-Monat als Spalte. Die Werte werden bewusst auf 1 oder 0 gesetzt, wobei 1 dafuer steht, dass eine Transaktion gemacht wurde in diesem Jahr-Monat und 0 fuer keine. 

```{r}
# select from buyers only account_id and year_month, group by and count number of rows, if there is a transaction in this year-month set value to 1 else 0
buyers_pivot <- buyers %>%
  select(account_id, year_month) %>%
  group_by(account_id, year_month) %>%
  summarize(count = n(), .groups = "drop") %>%
  mutate(value = 1) %>%
  pivot_wider(
    id_cols = account_id,
    names_from = year_month,
    values_from = value,
    values_fill = 0
  ) %>%
  select(account_id, sort(buyers$year_month))


buyers_pivot

# calculate the rowsum of each row and add them as a new column, sort ascending
buyers_pivot %>%
  mutate(rowsum = rowSums(across(-account_id))) %>%
  arrange(rowsum)

# ueberlegung, ob kauefer die ein weniger als 12 rowsums haben gedroppt werden. (ca. 150 Kauefer)
```

## 3.4 Roll-up visualization
Wir schreiben eine Funktion die uns erlaubt nach beliebige accounts zu filtern die eine Karte besitzen und in unserem Roll-up Fenster sind. Anschliessend fuehren wir eine Visualisierung von balance durch und zeichnen noch zusaetzlich ein rollingwindow von 30 Tagen ein. Wichtig dabei zu beachten ist, dass das lag-Monat nicht mit visualisiert wird! Diese Visualisierung erlaubt es uns, Kunden mit dem Produkt einer Karte besser zu verstehen. 

```{r}
visualize_account_balance <- function(transactions,
                                      accounts_to_filter) {
  # Filter transactions by account
  filtered_transactions <- transactions %>%
    filter(account_id %in% accounts_to_filter)

  # Check if filtered data frame is empty
  if (nrow(filtered_transactions) == 0) {
    warning("The provided account_id(s) do not appear in the transactions dataset.")
    return(NULL)
  }

  # visualisere balanace and rolling mean
  plot <- ggplot(filtered_transactions, aes(x = date)) +
    geom_line(aes(
      y = balance,
      color = as.factor(account_id)
    )) +
    labs(
      title = "Balance vs. Date",
      x = "Date",
      y = "Balance",
      color = "Account ID"
    )

  return(plot)
}

visualize_account_balance(buyers,
  accounts_to_filter = c(7, 14)
)
```


## 3.4 EDA von Karten Kaeufer

Wir schauen uns an, was ueberhaupt ein Kartenkaeufer ausmacht und explorieren den komibinerten Datensatz
```{r fig.width=10, fig.height=10}
# loan?
# create a barplot and count has_loan
data %>%
  ggplot(aes(
    x = has_loan.loan,
    fill = has_card.card
  )) +
  geom_bar(position = "dodge") +
  labs(
    title = "Barplot von Kredit",
    x = "Kredit vorhanden?",
    y = "Anzahl"
  )

# issuance_statement_frequency.account?
# create a barplot and count issuance_statement_frequency.account
data %>%
  ggplot(aes(
    y = issuance_statement_frequency.account,
    fill = has_card.card
  )) +
  geom_bar(position = "dodge") +
  labs(
    title = "Barplot von issuance_statement_frequency.account",
    y = "issuance_statement_frequency.account",
    x = "Anzahl"
  )

# age distribution
data %>%
  ggplot(aes(
    x = age.client,
    fill = has_card.card
  )) +
  geom_histogram(bins = 35) +
  labs(
    title = "Histogramm von Alter",
    x = "Alter",
    y = "Anzahl"
  )

# has order?
data %>%
  ggplot(aes(
    x = has_order.order,
    fill = has_order.order
  )) +
  geom_bar(position = "dodge") +
  labs(
    title = "Barplot von Dauerauftrag",
    x = "Dauerauftrag vorhanden?",
    y = "Anzahl"
  )

# gender.client?
# create barplot for gender
data %>%
  ggplot(aes(
    x = gender.client,
    fill = has_card.card
  )) +
  geom_bar(position = "dodge") +
  labs(
    title = "Barplot nach Geschlecht",
    x = "Geschlecht",
    y = "Anzahl"
  )

# district_name.district?
data %>%
  ggplot(aes(
    y = district_name.district,
    fill = has_card.card
  )) +
  geom_bar(position = "dodge") +
  labs(
    title = "Barplot nach Distrikt",
    x = "Anzahl",
    y = "Distrikt"
  )



# region.district?
data %>%
  ggplot(aes(
    y = region.district,
    fill = has_card.card
  )) +
  geom_bar(position = "dodge") +
  labs(
    title = "Barplot nach Region",
    x = "Anzahl",
    y = "Region"
  )
```


## 3.5 Testing

Wir versuchen aus den Transaktionsdaten von den buyer die Attribute in einem geeignetem Format zu kriegen. Dabei unterscheiden wir die Kategorische und Numerische Spalten. Grundsaetzlich gruppieren wir nach account_id und year_month und zaehlen bei den kategorischen variabeln wie oft diese variable vorkommt. Bei den numerischen Werten berechnen wir Attribute wie Summe, Mittelwert, Median etc.... anschliessend joinen wir alles zu einem Dataframe und pivotieren diese. Beim pivotieren erhalten wir zu viele Spalten (mehr als 1000). (Behalten den Code, vlt brauchen wir den spaeter)
```{r}
# Extracting year and month from the date column
buyers$year_month <- format(as.Date(buyers$date), "%Y-%m")

buyers

# Handling categorical datatypes in transaction

trans_type <- buyers %>%
  group_by(account_id, year_month, type) %>%
  summarise(count_type = n()) %>%
  pivot_wider(names_from = type, values_from = count_type, values_fill = list(count_type = 0)) %>%
  rename(
    "n_credit.trans.type" = "CREDIT",
    "n_withdrawl.trans.type" = "WITHDRAWAL",
    "n_NA.trans.type" = "NA"
  )

trans_type

trans_operation <- buyers %>%
  group_by(account_id, year_month, operation) %>%
  summarise(count_operation = n()) %>%
  pivot_wider(names_from = operation, values_from = count_operation, values_fill = list(count_operation = 0)) %>%
  rename(
    "n_withdrawal_in_cash.trans.operation" = "WITHDRAWAL IN CASH",
    "n_NA.trans.operation" = "NA",
    "n_credit_in_cash.trans.operation" = "CREDIT IN CASH",
    "n_remittance_to_anoth_bank.trans.operation" = "REMITTANCE TO ANOTHER BANK",
    "n_collec_from_anoth_bank" = "COLLECTION FROM ANOTHER BANK"
  )

trans_operation

trans_k_symbol <- buyers %>%
  group_by(account_id, year_month, k_symbol) %>%
  summarise(count_type = n()) %>%
  pivot_wider(names_from = k_symbol, values_from = count_type, values_fill = list(count_type = 0)) %>%
  rename(
    "n_interest_credited.trans.ksymbol" = "INTEREST CREDITED",
    "n_statement_payment.trans.ksymbol" = "STATEMENT PAYMENT",
    "n_NA.trans.ksymbol" = "NA",
    "n_houshold.trans.ksymbol" = "HOUSHOLD",
    "n_insurrance_pay.trans.ksymbol" = "INSURRANCE PAYMENT",
    "n_loan_pay.trans.ksymbol" = "LOAN PAYMENT",
    "n_old_age_pension.trans.ksymbol" = "OLD-AGE PENSION",
    "n_interes_if_neg_balance.trans.ksymbol" = "INTERES IF NEGATIVE BALANCE",
  )

trans_k_symbol

# handling numerical values
trans_amount_balance <- buyers %>%
  group_by(account_id, year_month) %>%
  summarise(
    sum_amount.trans = sum(amount, na.rm = TRUE),
    mean_amount.trans = mean(amount, na.rm = TRUE),
    median_amount.trans = median(amount, na.rm = TRUE),
    min_amount.trans = min(amount, na.rm = TRUE),
    max_amount.trans = max(amount, na.rm = TRUE),
    sum_balance.trans = sum(balance, na.rm = TRUE),
    mean_balance.trans = mean(balance, na.rm = TRUE),
    median_balance.trans = median(balance, na.rm = TRUE),
    min_balance.trans = min(balance, na.rm = TRUE),
    max_balance.trans = max(balance, na.rm = TRUE),
    num_of_trans.trans = n(),
  )

trans_amount_balance

# Join everthing together into one big dataframe
trans_aggregated <- inner_join(trans_type, trans_operation, by = c("account_id", "year_month")) %>%
  inner_join(trans_k_symbol, by = c("account_id", "year_month")) %>%
  inner_join(trans_amount_balance, by = c("account_id", "year_month"))

trans_aggregated <-
  trans_aggregated

trans_aggregated_wider <- trans_aggregated %>%
  pivot_wider(
    names_from = year_month,
    values_from = -c(account_id, year_month)
  )

trans_aggregated_wider
```

# 4. Nicht-Käufer
Bestimmen der Nicht-Käufer zum Vergleich (inkl. Rollup-Fenster).

## 4.1 Bestimmung von Nicht-Käufer

Wir haben 3094 potenzielle Kunden denen wir eine Karte anbieten könnten.

(Kontaktstunde vom 24.10.2023)
Weiter schauen wir uns an, wann der account erstellt wurde und berechnen 12 Monate in die "zukunft", damit wir Neukunden nicht als potenzielle Kunden betrachten, da deren Verhalten am Anfang ein anderer ist, als bei einem langjaehrigen Kunden. 
```{r}
# create a dataframe where has_card.card == FALSE
account_ids_no_cards <- data %>%
  filter(has_card.card == FALSE) %>%
  select(account_id, account_creation_date.account)

account_ids_no_cards
```

## 4.2 Fiktives issued für Nicht-Käufer definieren

Erste Idee:
Idee: Wir haben einen pool von non_buyers_data und einen pool von buyers_data, welche ein issued.card Datum haben. 
Nun wollen wir für einen non-buyer einen möglichst ähnlichen buyer finden und nehmen dann diesen issue.card Datum und fügen diese dann dem non-buyer hinzu. 
Dabei soll berücksichtigt werden, dass: 
- Zeitspanne (issued.date - 13 Monate -> Transkationen bei non-buyers vorhanden?
- Alter ? 
- Geschlecht ? 
- Region ? 
- has_loan ?
- has_order ?
- issuance_statement_frequency.account)

wie? 
Iteriere durch non-buyers account, filtere ob bsp. Geschlecht gleich ist, Region gleich ist, has_loan gleich ist, has_order oder issuance_statement. Es bleiben nur noch die buyers übrig die diesen krieterien entsprechen mit dem non-buyer. Anschliessend vergleiche, ob der non-buyer und buyer mindestens 12 Monate überlappngen aufweist, falls ja, berechne nun das Alter, und das Alter mit der tiefsten Differenz, von dem buyer nimm das issued.card Datum und füge das dem account_ids_no_cards zur entsprechenden non-buyer id hinzu. 

Feedback von Dani (Kontaktstunde 24.10.2023):
Idee nicht gut, da wir dann das Modell die Möglichkeit vorweg schon weg nehmen aufgrund von den Attributen zu lernen.  

Neue idee: 
Von Buyers Transaktionsdataframe in dem Rollup Fenster erstellen. (issued.card - 13 Monate bis start_lag_date)
buyers Transaktionsdataframe, vom Datum -> Jahr und Monat extrahiere und dann pivotiere. 
Anschliessend mit 1 oder 0 auffuellen. (Siehe 3.3)

Analog mache ich das gleiche fuer non buyers, filtere aber jedoch nicht das Fenster. 
Anschliessend picke ich random einen buyer aus und nehme den grössten jaccard wert mit non buyer in bezug auf die transkationsdaten jahr monat. Falls ein Non-Buyer mehrere aehnliche Buyers zugewiesen werden kann, so wird random ein Buyer gepickt und deren issued.date dem Non-Buyer uebertragen.

```{r}
# get transaktion dataframe from non-buyers
non_buyers <- inner_join(
  x = account_ids_no_cards,
  y = trans,
  by = "account_id"
)

# extract month
non_buyers$year_month <- format(as.Date(non_buyers$date), "%Y-%m")

non_buyers
```

### 4.2.1 Fenster von Nicht Kauefer bearbeiten

(Kontaktstunde vom 24.10.2023)
Wir filtern die Transaktionsdaten, damit wir die Transkationsdaten von Neukunden (Kunden die im ersten Jahr bei der Bank sind), nicht haben. 

```{r}
# create a column where you add one year to creation date
non_buyers <- non_buyers %>%
  mutate(
    end_date = account_creation_date.account %m+% months(12),
  )

# Exclude date range between account_creation_date.account and end_date from non_buyers
non_buyers <- non_buyers %>%
  filter(date < account_creation_date.account | date > end_date)

# reorder dataframe non_buyers
non_buyers <- non_buyers %>%
  select(account_id, date, year_month, type:bank)

non_buyers
```

### 4.2.2 Non-buyer pivotieren
Wir pivotieren die non-buyers, analog wie bei den buyers. Jede Spalte repraesntiert das Jahr und den Monat, die Werte 1 oder 0 sagen aus, ob eine Transaktion durchgefuehrt wurde oder nicht. Da wir beim pivotieren nicht die gleiche Anzahl an spalten erhalten, wurde noch von den Transaktions Dataset das minimale und maximale Datum herausgefiltert und eine Liste gebastelt mit einem Abstand von jeweils einen Monat. Anschliessend ueberpruefen wir, ob das Jahr-Monat vorkam, falls nicht wird eine Spalte erstellt und diese mit 0 aufgefuellt. wir sortieren beide Dataframes und erhalten somit die gleiche Anzahl spalten fuer die buyers (im Roll-up Fenster) und non-buyers. 

```{r}
# pivot non-buyers
non_buyers_pivot <- non_buyers %>%
  select(account_id, year_month) %>%
  group_by(account_id, year_month) %>%
  summarize(count = n(), .groups = "drop") %>%
  mutate(value = 1) %>%
  pivot_wider(
    id_cols = account_id,
    names_from = year_month,
    values_from = value,
    values_fill = 0
  ) %>%
  select(account_id, sort(non_buyers$year_month))

# extract from trans date the min and max date but just year-month
min_date <- min(format(as.Date(trans$date), "%Y-%m"))
max_date <- max(format(as.Date(trans$date), "%Y-%m"))

# append "-01" to min_date and max_date
min_date <- paste(min_date, "-01", sep = "")
max_date <- paste(max_date, "-01", sep = "")

# create an array between min_date and max_date 1 months apart
date_range <- seq.Date(from = as.Date(min_date), to = as.Date(max_date), by = "month")

year_month_range <- format(date_range, "%Y-%m")

# if there is a column that does not exist in non_buyers_pivot and buyer but in year_month_range, add it and set value to 0
for (col in year_month_range) {
  if (!(col %in% colnames(non_buyers_pivot))) {
    non_buyers_pivot[[col]] <- 0
  }
}

# same for buyers_pivot
for (col in year_month_range) {
  if (!(col %in% colnames(buyers_pivot))) {
    buyers_pivot[[col]] <- 0
  }
}

# sort both dataframe by year_month range
non_buyers_pivot <- non_buyers_pivot %>%
  select(account_id, sort(year_month_range))

buyers_pivot <- buyers_pivot %>%
  select(account_id, sort(year_month_range))

buyers_pivot
non_buyers_pivot
```


## 4.3 Bestimmung von fiktives issued_date
Mit den beiden Dataframes aus 4.2.2 koennen wir nun den Jaccard-index berechnen. Sprich die groesste Zeitfensterueberlappungen von Roll-up Fenster Buyer und non-buyers. Da es sein kann, dass mehrere ueberlappungen von non-buyers und buyers gleich sind, picken wir random ein issued date in diesem fall vom buyer aus und fuegen dem den non-buyer hinzu. 

```{r}
# WICHTIG: Sortieren nach account_id aufsteigend!

# sort all dataframes by account_id ascending with a function
sort_by_account_id <- function(df) {
  df <- df %>%
    arrange(account_id)
}

# apply function to all 4 dataframes in this codeblock
non_buyers_pivot <- sort_by_account_id(non_buyers_pivot)
buyers_pivot <- sort_by_account_id(buyers_pivot)
account_ids_has_card <- sort_by_account_id(account_ids_has_card)
account_ids_no_cards <- sort_by_account_id(account_ids_no_cards)

account_ids_has_card
account_ids_no_cards
buyers_pivot
non_buyers_pivot
```

### 4.3.1 Umwandeln in Matrizen
Wir wandeln die buyers_pivot un non_buyers_pivot in matrizen um, und droppen dabei account_id, da diese bei der Berechnung von Jaccard nicht dabei sein sollte. Aus diesem Grund war das sortieren vorhin sehr wichtig, damit spaeter eine Zuweisung wieder auf deren Indexe gemacht werden kann. 
```{r}
# drop in buyers_pivot and non_buyers_pivot
buyers_pivot_matrix <- buyers_pivot %>%
  select(-c(account_id)) %>%
  as.matrix()

buyers_pivot_matrix

# select one row from buyers_pivot_matrix
# buyers_pivot_matrix[1, 1:ncol(buyers_pivot_matrix)]


non_buyers_pivot_matrix <- non_buyers_pivot %>%
  select(-c(account_id)) %>%
  as.matrix()

non_buyers_pivot_matrix
```

### 4.3.2 Jaccard Index Testing
Hier testen wir den Jaccard Index und schreiben eine Funktion dafuer.
```{r}
# Define two binary matrices
matrix1 <- matrix(c(
  1, 0, 0, 1,
  0, 1, 1, 0,
  1, 1, 0, 1
), nrow = 3, byrow = TRUE)

matrix2 <- matrix(c(
  0, 1, 0, 1,
  1, 0, 1, 0,
  1, 1, 1, 0
), nrow = 3, byrow = TRUE)

# Define function to calculate Jaccard Index for two rows
jaccard_index <- function(row1, row2) {
  intersection <- sum(row1 * row2)
  union <- sum(row1 + row2 > 0)
  return(intersection / union)
}

# Calculate row-wise Jaccard Index for each combination of rows
nrows1 <- nrow(matrix1)
nrows2 <- nrow(matrix2)
jaccard_matrix <- matrix(0, nrow = nrows1, ncol = nrows2)
for (i in 1:nrows1) {
  for (j in 1:nrows2) {
    jaccard_matrix[i, j] <- jaccard_index(matrix1[i, ], matrix2[j, ])
  }
}

# Print the result
print(jaccard_matrix)
```

### 4.3.3 Anwendung Jaccard index
Wir wenden die Funktion an unserem echten Datensatz und erhalten eine 691x3086 Jaccard Matrix. 691 Zeilen von den buyers und 3086 von den Non Buyers. 
```{r}
# Calculate row-wise Jaccard index for each combination of rows with buyers_pivot_matrix and non_buyers_pivot_matrix
nrows1 <- nrow(buyers_pivot_matrix)
nrows2 <- nrow(non_buyers_pivot_matrix)
jaccard_matrix <- matrix(0, nrow = nrows1, ncol = nrows2)
for (i in 1:nrows1) {
  for (j in 1:nrows2) {
    jaccard_matrix[i, j] <- jaccard_index(buyers_pivot_matrix[i, ], non_buyers_pivot_matrix[j, ])
  }
}

# print nrow and ncol of jaccard_matrix
print(nrow(jaccard_matrix))
print(ncol(jaccard_matrix))
```

### 4.3.4 Indexe
Da wir 3086 Spalten haben, muessten wir diese nun in Spalten betrachten, um den hoechsten Jaccard Wert und der zugehoerigen Index aus der Zeile herauszufinden. Wir koennen somit fuer jeden non-buyer den hochsten Wert und dem entsprechenden buyer herausholen, indem wir den index der zeile nehmen und diesen den issued.date nachschauen und dies dann im index von der Spalte im non-buyer account hinzufuegen.
```{r}
# for each column, get the highstes value and its corresponding row index and column index in a dataframe.
jaccard_df <- data.frame()
for (i in 1:ncol(jaccard_matrix)) {
  col_values <- jaccard_matrix[, i]
  # höchster Spaltenwert
  max_value <- max(col_values)
  # alle row indexe
  max_indices <- which(col_values == max_value)
  # eines davon auswaehlen random
  selected_index <- sample(max_indices, 1)

  # hinzufuegen zum dataframe
  jaccard_df <- rbind(jaccard_df, data.frame(
    row_index = selected_index,
    col_index = i,
    jaccard_value = max_value
  ))
}

# rename row_index to buyers_card_index and col_index to non_buyers_card index
jaccard_df <- jaccard_df %>%
  rename(
    buyers_card_index = row_index,
    non_buyers_card_index = col_index
  )

jaccard_df

# Create row index in account_ids_has_card and account_ids_no_cards
account_ids_has_card <- account_ids_has_card %>%
  mutate(
    row_index = row_number()
  ) %>%
  select(buyers_card_index = row_index, account_id:start_lag_date)

account_ids_has_card

account_ids_no_cards <- account_ids_no_cards %>%
  mutate(
    row_index = row_number()
  ) %>%
  select(non_buyers_card_index = row_index, account_id:account_creation_date.account)

account_ids_no_cards
```

### 4.3.5 Fiktives Issued Date
Hier fuegen wir das fiktive issued date zu unserem account_ids_no_cards dataframe hinzu indem wir auf deren indexe zugreifen.
```{r}
# inner join jaccard_df with account_ids_no_cards by non_buyers_card_index
non_buyer_with_buyers_card_index <- inner_join(
  x = account_ids_no_cards,
  y = jaccard_df,
  by = "non_buyers_card_index"
) %>%
  select(-c(non_buyers_card_index, account_creation_date.account)) %>%
  rename("non_buyer_account_id" = "account_id")

non_buyer_with_buyers_card_index

# left join account_ids_no_cards with account_ids_has_card by buyers_card_index
non_buyers_fictive_date <- left_join(
  x = non_buyer_with_buyers_card_index,
  y = account_ids_has_card,
  by = "buyers_card_index"
) %>%
  select(-c(start_date, start_lag_date, buyers_card_index)) %>%
  rename(
    "buyer_account_id" = "account_id",
    "fictive_issued.card" = "issued.card"
  ) %>%
  select(non_buyer_account_id, fictive_issued.card, jaccard_value, buyer_account_id)

non_buyers_fictive_date %>% arrange(desc(jaccard_value))

non_buyers_fictive_date %>% arrange(non_buyer_account_id)
```


```{r}
non_buyers_fictive_date
```
```{r}
data
```


### 4.3.6 Fiktives Kaufdatum in grosses Dataframe hinzufuegen
```{r}
# impute issued.card NA values in data with fictive_issued.card value by first column account_id
data <- left_join(data, non_buyers_fictive_date, by = c("account_id" = "non_buyer_account_id")) %>%
  mutate(
    issued.card = coalesce(issued.card, fictive_issued.card)
  ) %>%
  select(-c(fictive_issued.card, jaccard_value, buyer_account_id))

sum(is.na(data$issued.date))

data
```

## 4.4 Vergleich 
Visualisierungstest von non_buyer_account_id = 91 und buyer_account_id 3725, mittels Funktion aus 3.4

Die Funnktion visualisiert, dass die Ueberlappung von non-buyer und buyer funktioniert hat und der Code von 4.3 funktioniert! 
```{r}
visualize_account_balance(trans,
  accounts_to_filter = c(91, 3725)
)

visualize_account_balance(trans,
  accounts_to_filter = c(1390, 2409)
)
```

## 4.5 Rollup-Fenster Vergleich
Wir selektieren von den Non-Buyers die account_id und das fiktive Datum, erstellen die Roll-up Fenster analog zu den Buyer und filtern die trans Dataset in den entsprechenden Roll-up Fenster und vergleichen diese dann mit der visualisierungs Funktion fuer die gleichen Accounts wie bei 4.3.4

```{r}
# als Vergleich account_ids_has_card
account_ids_has_card <- account_ids_has_card %>% select(account_id:start_lag_date)
account_ids_has_card

# Create analoges Dataframe
account_ids_without_card <- non_buyers_fictive_date %>%
  select(account_id = non_buyer_account_id, fictive_issued.card) %>%
  mutate(
    start_date = fictive_issued.card %m-% months(13),
    start_lag_date = fictive_issued.card %m-% months(1)
  )


account_ids_without_card

non_buyers <- inner_join(
  x = account_ids_without_card,
  y = trans,
  by = "account_id"
) %>%
  filter(
    date >= start_date & date <= start_lag_date
  ) %>%
  select(-c(start_date, start_lag_date)) %>%
  mutate(year_month = format(as.Date(date), "%Y-%m")) %>%
  select(account_id, issued.card = fictive_issued.card, date, year_month, type:bank)


# print transaction table from buyers and non-buyers
buyers
non_buyers

# create transaction_roll_up window for buyers and non-buyers by concat buyers and non_buyers dataframe
trans_within_range <- rbind(buyers, non_buyers)
trans_within_range

visualize_account_balance(trans_within_range,
  accounts_to_filter = c(91, 3725)
)

visualize_account_balance(trans_within_range,
  accounts_to_filter = c(1390, 2409)
)
```

# 5 Recap
Da wir bis zu diesem Schritt vieles gemacht haben, gibt es hier ein kurzes Re-Cap, um die Übersicht wieder zu gewinnen, falls man diese bis dahin verloren hat. 

In den Abschnitten 1 bis und mit 1.2.8 haben wir die Daten eingelesen und versucht einen Zusammenhang der Tabellen zu verstehen, gegebenenfalls auch Data Wrangling betrieben. 

Im Abschnitt 1.3 - 1.3.2 wurden gegebenenfalls die Na Werte imputiert und eine Explorative Datenanalyse durchgeführt.

Im Abschnitt 1.4 haben wir Junioren Karten entfernt und auch eine Altersgrenze definiert, in denen sich darüber unsere Potenzielle Kunden befinden.

Im Abschnitt 2 bis und mit 2.9 haben wir die Tabellen miteinenader verknüpft und ein grosses Dataframe erhalten, welches `data` heisst. 

Im Abschnitt 2.10 - 3.5 Haben wir bei den Transaktionellen Dataframe die wichtigsten Features extrahiert, von den Kunden die eine Karte besitzt ein Roll-up Fenster inklusivem Lag Fenster bestummen und die Transkationsdaten entsprechend in dem Zeitbereich gefiltert und Funktionen zur Visualisierung der Balance geschrieben. 

Im Abschnitt 4 bis und mit 4.3.5 haben wir ein fiktives issued.date für unsere potenziellen Kunden die keine Karte besitzen bestummen, indem wir das Roll-up Fenster von Kunden pivotiert haben auf Jahr-Monat und für vorhandene Transaktion eine 1 hinzugefügt haben und eine 0 für keine Transaktion, anschliessend analog das gleiche mit den potenziellen Kunden getan und dann den Jaccard Index berechnet und daraus von den potenziellen Kunden, ein fiktives issued.date abgeleitet. Ein wichtiger Punkt dabei war das pivotieren in Jahr-Monat, sowie die gleiche Anzahl an Spalten von den Karten Besitzer und potenziellen Kunden zu haben. Ein weitere wichtiger Punkt war, dass wir die account_ids für die Kartenbesitzer wie auch potenzielle Kunden indexiert haben, da diese bei der Jaccard-Berechnungen stören und anschliessend aufgrund vom Index auf unsere account_ids der Kartenbesitzer wie auch potenzielle Kunden wieder zugewiesen haben. Damit in diesem wichtigen Schritt keine Falsche Zuweisungen stattfinden, wurde davor nach account_ids aufsteigend sortiert. Weiter gab es mehrere gleiche maximal höchste Jaccard-Werte für einen potenzielle Kunden mit mehreren Kartenbesitzer, um dieses Problem zu lösen, wurde dann random ein Kartenbesitzer als Vergleich genommen. 

Abschnitt 4.4 bis 4.5 dient als Vergleich und Data Wrangling der Transaktionellen Daten für die potenziellen Kunden sowie deren Vergleich zu den Kartenbesitzer. 

Die wichtigsten Dataframes die wir brauchen sind:
`data` => grosses gejointes Dataframe
`buyers` => Transkations Dataframe von Kartenbesitzer innerhalb des Roll-up Fenster
`non_buyers` => Transkations Dataframe von potenziellen Kunden innerhalb des Roll-up Fenster


## 5.1 Speicher Putzen
Wir bereinigen den Speicher indem wir alle Daten und Variabeln entfernen die wir nicht mehr benötigen, da ich ansonsten Probleme mit dem Zwischenspeicher erhalte und das Notebook langsam wird.
```{r}
all_objects <- ls()
remove_objects <- setdiff(all_objects, c("data", "buyers", "non_buyers", "trans"))
rm(list = remove_objects)
print(ls())
```

## 5.2 Export der Daten
```{r}
write_parquet(data, "data_export/data.parquet")
write_parquet(trans, "data_export/trans.parquet")
write_parquet(buyers, "data_export/buyers.parquet")
write_parquet(non_buyers, "data_export/non_buyers.parquet")
```

## 5.3 Import der Daten
Damit wir nicht immer das Notebook neu laufen müssen werden hier die Daten wieder eingelesen. 
```{r}
data <- read_parquet("data_export/data.parquet")
trans <- read_parquet("data_export/trans.parquet")
buyers <- read_parquet("data_export/buyers.parquet")
non_buyers <- read_parquet("data_export/non_buyers.parquet")

data
trans
buyers
non_buyers
```


# 6. Event Bezogene Kundeninformationen 
Erzeugen event-bezogener Kundeninformationen für 12 Monate vor 
Kreditkartenkauf (analog für Nicht-Käufer).

## 6.1 Kundeninformationen extrahieren
Wir schreiben hier Funktionen die uns erlaubt, Kundeninformationen aus den Transaktions Dataframes für Käufer und Nicht-Käufer zu extrahieren.
(Vermögen und Umsätze werden in Abschnitt 7. behandelt)

```{r}
buyers
non_buyers
```

### 6.1.1 Funktion Suffix
```{r}
# Add sufix to column name
add_suffix <- function(data, suffix) {
  colnames(data) <- paste(colnames(data), suffix, sep = ".")
  return(data)
}

buyers <- add_suffix(buyers, "trans")
buyers
non_buyers <- add_suffix(non_buyers, "trans")
non_buyers
```

### 6.1.2 Funktion Anzahl Transaktion pro Jahr-Monat
Extrahieren der Anzahl Transaktionen pro Jahr-Monat. 
```{r}
# Create a function where you extract from column year_month the number of transaktion (count rows)
extract_count <- function(data, trans) {
  data <- data %>%
    group_by(account_id.trans, year_month.trans) %>%
    summarise(count = n(), .groups = "drop") %>%
    pivot_wider(
      id_cols = account_id.trans,
      names_from = year_month.trans,
      values_from = count,
      values_fill = 0
    )

  # extract from trans date the min and max date but just year-month
  min_date <- min(format(as.Date(trans$date), "%Y-%m"))
  max_date <- max(format(as.Date(trans$date), "%Y-%m"))

  # append "-01" to min_date and max_date
  min_date <- paste(min_date, "-01", sep = "")
  max_date <- paste(max_date, "-01", sep = "")

  # create an array between min_date and max_date 1 months apart
  date_range <- seq.Date(from = as.Date(min_date), to = as.Date(max_date), by = "month")
  year_month_range <- format(date_range, "%Y-%m")

  # if there is a column that does not exist in non_buyers_pivot and buyer but in year_month_range, add it and set value to 0
  for (col in year_month_range) {
    if (!(col %in% colnames(data))) {
      data[[col]] <- 0
    }
  }

  # sort datafarme
  data <- data %>%
    select(account_id.trans, sort(year_month_range))

  return(data)
}
```

### 6.1.3 Funktion Kategorische Variablen
```{r}
# Groupby account_id and count the number of occurenc for a catergorical variable
extract_count_categorical <- function(data, col_name) {
  col_name_sym <- as.name(col_name)

  data %>%
    mutate(!!col_name_sym := if_else(!!col_name_sym == "", "Unknown", !!col_name_sym)) %>%
    group_by(account_id.trans, !!col_name_sym) %>%
    summarise(count = n(), .groups = "drop") %>%
    pivot_wider(
      id_cols = account_id.trans,
      names_from = !!col_name_sym,
      values_from = count,
      values_fill = 0
    )
}
```


## 6.2 Kundeninformation für 12 Monate von Käufer
```{r}
# extract number of transaction for year_month
info_buyers <- extract_count(buyers, trans)
# add sufix .n_trans
info_buyers <- add_suffix(info_buyers, "n_trans") %>% rename("account_id.trans" = "account_id.trans.n_trans")

info_buyers

info_cat_buyers <- extract_count_categorical(buyers, "type.trans") %>%
  rename(
    "type.credit.n_trans" = "CREDIT",
    "type.withdrawal.n_trans" = "WITHDRAWAL",
    "type.unkown.n_trans" = "NA"
  ) %>%
  inner_join(extract_count_categorical(buyers, "operation.trans") %>%
    rename(
      "opera.credit_in_cash.n_trans" = "CREDIT IN CASH",
      "opera.remitt_to_anoth_bank_.n_trans" = "REMITTANCE TO ANOTHER BANK",
      "opera.withdraw_in_cash.n_trans" = "WITHDRAWAL IN CASH",
      "opera.unkown.n_trans" = "NA",
      "opera.collect_from_anoth_bank.n_trans" = "COLLECTION FROM ANOTHER BANK"
    ), by = "account_id.trans") %>%
  inner_join(
    extract_count_categorical(buyers, "k_symbol.trans") %>%
      rename(
        "k_symbol.houshold.n_trans" = "HOUSHOLD",
        "k_symbol.interest_credited.n_trans" = "INTEREST CREDITED",
        "k_symbol.statement_pay.n_trans" = "STATEMENT PAYMENT",
        "k_symbol.unkown.n_trans" = "NA",
        "k_symbol.insurance_pay.n_trans" = "INSURRANCE PAYMENT",
        "k_symbol.loan_pay.n_trans" = "LOAN PAYMENT",
        "k_symbol.old_age_pension.n_trans" = "OLD-AGE PENSION",
        "k_symbol.interest_if_negativ_balance.n_trans" = "INTERES IF NEGATIVE BALANCE"
      ),
    by = "account_id.trans"
  )

info_cat_buyers

info_buyers <- inner_join(info_buyers, info_cat_buyers, by = "account_id.trans")

info_buyers
```


## 6.3 Kundeninformation für 12 Monate von Nicht-Käufer
```{r}
# extract number of transaction for year_month
info_non_buyers <- extract_count(non_buyers, trans)
# add sufix .n_trans
info_non_buyers <- add_suffix(info_non_buyers, "n_trans") %>% rename("account_id.trans" = "account_id.trans.n_trans")

info_non_buyers

info_cat_non_buyers <- extract_count_categorical(non_buyers, "type.trans") %>%
  rename(
    "type.credit.n_trans" = "CREDIT",
    "type.withdrawal.n_trans" = "WITHDRAWAL",
    "type.unkown.n_trans" = "NA"
  ) %>%
  inner_join(extract_count_categorical(non_buyers, "operation.trans") %>%
    rename(
      "opera.credit_in_cash.n_trans" = "CREDIT IN CASH",
      "opera.remitt_to_anoth_bank_.n_trans" = "REMITTANCE TO ANOTHER BANK",
      "opera.withdraw_in_cash.n_trans" = "WITHDRAWAL IN CASH",
      "opera.unkown.n_trans" = "NA",
      "opera.collect_from_anoth_bank.n_trans" = "COLLECTION FROM ANOTHER BANK"
    ), by = "account_id.trans") %>%
  inner_join(
    extract_count_categorical(non_buyers, "k_symbol.trans") %>%
      rename(
        "k_symbol.houshold.n_trans" = "HOUSHOLD",
        "k_symbol.interest_credited.n_trans" = "INTEREST CREDITED",
        "k_symbol.statement_pay.n_trans" = "STATEMENT PAYMENT",
        "k_symbol.unkown.n_trans" = "NA",
        "k_symbol.insurance_pay.n_trans" = "INSURRANCE PAYMENT",
        "k_symbol.loan_pay.n_trans" = "LOAN PAYMENT",
        "k_symbol.old_age_pension.n_trans" = "OLD-AGE PENSION",
        "k_symbol.interest_if_negativ_balance.n_trans" = "INTERES IF NEGATIVE BALANCE"
      ),
    by = "account_id.trans"
  )

info_cat_non_buyers

info_non_buyers <- inner_join(info_non_buyers, info_cat_non_buyers, by = "account_id.trans")

info_non_buyers
```

## 6.4 Kundenalter bei Kaufdatum
Wir berechnen direkt das Alter zur gegebenen issued.card Zeit indem wir das grosses Dataframe "data" mit einer weiteren Spalte erweitern namens: age_issued.card, dies erreichen wir indem wir vom Geburtstdatum und issued.card Datum das Alter heraus extrahieren.
```{r}
data$age_issued.card <- as.integer(floor(difftime(data$issued.card, data$birht_number.client, units = "days") / 365.25))

data
```


## 6.5 Resultate der Kundeninformationen
```{r}
info_buyers
info_non_buyers
data %>% select(age_issued.card)
```


# 7. Vermögen & Umsätze
Konstruieren der Vermögen und Umsätze im Rollup-Fenster auf Basis 
der Transaktionshistorie.

```{r}
unique(non_buyers$operation.trans)
```

## 7.1 Bestimmung von Gutschrift und Belastung
Es gibt Gesamthaft 5 Transaktions Operationen, 
Gutschriften sind folgende Operationen: NA, Collection from Another Bank, Credit in cash
Belastungen sind folgende Operationen: Withdrawal in Cash, Remittance to Another Bank
In der Nachfolgende Funktion extrahieren wir von der Spalte Amount -> Gutschrift und Belastung.

```{r}
gutschrift_belastung <- function(df) {
  df <- df %>%
    mutate(
      gutschrift.trans = case_when(
        operation.trans %in% c(NA, "COLLECTION FROM ANOTHER BANK", "CREDIT IN CASH") ~ amount.trans,
        TRUE ~ 0
      ),
      belastung.trans = case_when(
        !operation.trans %in% c(NA, "COLLECTION FROM ANOTHER BANK", "CREDIT IN CASH") ~ amount.trans,
        TRUE ~ 0
      )
    ) %>%
    select(-c(amount.trans))
  return(df)
}

non_buyers_g_b <- gutschrift_belastung(non_buyers)
buyers_g_b <- gutschrift_belastung(buyers)

non_buyers_g_b
buyers_g_b
```


## 7.2 Vermögen

Bei genauerem betrachten der Transaktionshistorie von mehreren Kunden ist mir aufgefallen, dass die letzte Transkation im Monat immer die von der Bank selber ist. type.trans ist immer "CREDIT" und operations.trans ein NA Wert. Diese Art der Transkation markiert ein Enddatum des Monates und ist somit die letzte Transkation des Monates!

Bemerkt habe ich weiter, dass der Monatlicher Umsatz berechnet werden kann indem die Differenz vom Enddatum des Monates mit dem vorherhigen Enddatum des vorherhigen Monates berechnet werden kann. 

Wir kennen die letzte Transaktion des Monates und könnnen somit immer den Saldo ablesen. Hier filtern wir nach den bestimmten Transaktionen, lesen das Vermoegen aus und pivotieren anschliessend das Dataframe, damit wir auf einer Zeile direkt herauslesen können, welches Vermögen der Kunde am Ende des Monates aufweist. 


```{r}
balance_end_month <- function(df, display_df = FALSE) {
  df <- df %>%
    filter(is.na(operation.trans) & type.trans == "CREDIT") %>%
    select(account_id.trans,
      end_month = year_month.trans,
      balance.trans
    )

  if (display_df) {
    print(df)
  }

  return(df)
}

buyers_balance <- balance_end_month(buyers, display_df = TRUE)
non_buyers_balance <- balance_end_month(non_buyers, display_df = TRUE)
```

## 7.3 Umsätze

Hier aggregieren wir die Gutschrift und Belastungsspalten mittels einer kummulierten Summe und pivotieren diese anschliessend, dass wir aus einer Zeile den monatlichen Umsatz Gutschrift und Monatlichen Umsatz Belastung sehen. 

```{r}
umsatz_end_month <- function(df, display_df = FALSE) {
  result <- df %>%
    select(account_id.trans, year_month.trans, gutschrift.trans, belastung.trans) %>%
    group_by(account_id.trans, year_month.trans) %>%
    summarise(
      total_gutschrift = sum(gutschrift.trans),
      total_belastung = sum(belastung.trans),
      .groups = "drop"
    ) %>%
    rename("end_month" = "year_month.trans")

  if (display_df) {
    print(result)
  }

  return(result)
}

buyers_umsatz <- umsatz_end_month(buyers_g_b, display_df = TRUE)
non_buyers_umsatz <- umsatz_end_month(non_buyers_g_b, display_df = TRUE)
```


## 7.4 Umsatz und Vermögen zusammenjoinen

Warum haben wir eine Differenz zwischen dem buyers_balance und buyers_umsatz in Bezug auf ihre Anzahl Rows?
Antowort: Der Buyers Datensatz wurde durch das Roll-up Fenster manipuliert, welches durch das Kaufdatum - 13 Monate gerechnet wurde (Auf Tage genau und nicht Monate).
Bsp: Kaufdatum: 15.05.1999 -> Ergibt ein Rollup Fenster beginnend von 15.04.1998. 

Dadurch das wir nach type.trans == "CREDIT" und is.na(opeartion.trans) filtern, erhalten wir von dem jeweiligen Account_id immer die Transaktion in einem letzten Monat. Es kommt nun vor, dass wir dann für den aller letzten Monat im Rollup Fenster uns nicht am Ende eines Monates befinden und somit der Filter für den aller letzten Monat keine Balance auslesen kann. 

Beim Umsatz wiederum achten wir nicht darauf und summieren den Monat nach Gutschrift und Belastung. 

Das ist der Grund für die Differenz der Anzahl Rows bei den Dataframes.
Wir gehen nun so vor, dass wir nun einen left_join auf buyers_balance machen, denn dort sind wir sicher, dass wir nicht mehr als 12 Zeilen pro Account_id haben.
```{r}
buyers_balance_umsatz <- left_join(buyers_balance, buyers_umsatz, by = c("account_id.trans", "end_month"))

non_buyers_balance_umsatz <- left_join(non_buyers_balance, non_buyers_umsatz, by = c("account_id.trans", "end_month"))

buyers_balance_umsatz
non_buyers_balance_umsatz

write_parquet(buyers_balance_umsatz, "data_export/buyers_balance_umsatz.parquet")
write_parquet(non_buyers_balance_umsatz, "data_export/non_buyers_balance_umsatz.parquet")
```

# 8. Kunden-spezifische statistische Kennzahlen
Herleiten Kunden-spezifischer, statistischer Kennzahlen für Vermögen 
und Umsätze im Rollup-Fenster mittels Funktionen.

## 8.1 Funktion

```{r}
## Full Roll-up Window
summarize_transactions <- function(df) {
  df <- df %>% mutate(end_month = ymd(paste0(end_month, "-01")))
  df_aggregated <- df %>%
    group_by(account_id.trans) %>%
    summarize(
      min_balance = min(balance.trans, na.rm = TRUE),
      max_balance = max(balance.trans, na.rm = TRUE),
      mean_balance = mean(balance.trans, na.rm = TRUE),
      median_balance = median(balance.trans, na.rm = TRUE),
      sd_balance = sd(balance.trans, na.rm = TRUE),
      last_balance = last(balance.trans),
      trend_balance = tidy(lm(balance.trans ~ end_month, data = cur_data()))$estimate[2],
      spearman_balance = ifelse(sd(balance.trans, na.rm = TRUE) == 0, NA, cor(balance.trans, as.numeric(end_month), method = "spearman", use = "complete.obs")),
      min_gutschrift = min(total_gutschrift, na.rm = TRUE),
      max_gutschrift = max(total_gutschrift, na.rm = TRUE),
      mean_gutschrift = mean(total_gutschrift, na.rm = TRUE),
      median_gutschrift = median(total_gutschrift, na.rm = TRUE),
      sd_gutschrift = sd(total_gutschrift, na.rm = TRUE),
      last_gutschrift = last(total_gutschrift),
      trend_gutschrift = tidy(lm(total_gutschrift ~ end_month, data = cur_data()))$estimate[2],
      spearman_gutschrift = ifelse(sd(total_gutschrift, na.rm = TRUE) == 0, NA, cor(total_gutschrift, as.numeric(end_month), method = "spearman", use = "complete.obs")),
      min_belastung = min(total_belastung, na.rm = TRUE),
      max_belastung = max(total_belastung, na.rm = TRUE),
      mean_belastung = mean(total_belastung, na.rm = TRUE),
      median_belastung = median(total_belastung, na.rm = TRUE),
      sd_belastung = sd(total_belastung, na.rm = TRUE),
      last_belastung = last(total_belastung),
      trend_belastung = tidy(lm(total_belastung ~ end_month, data = cur_data()))$estimate[2],
      spearman_belastung = ifelse(sd(total_belastung, na.rm = TRUE) == 0, NA, cor(total_belastung, as.numeric(end_month), method = "spearman", use = "complete.obs"))
    )

  return(df_aggregated)
}

## With Window
summarize_transaction_window <- function(data, n_last_months, suffix_name) {
  df_aggregated <-
    data %>%
    group_by(account_id.trans) %>%
    mutate(end_month = ymd(paste(end_month, "-01"))) %>%
    filter(end_month >= max(end_month) - months(n_last_months)) %>%
    summarize(
      min_balance = min(balance.trans, na.rm = TRUE),
      max_balance = max(balance.trans, na.rm = TRUE),
      mean_balance = mean(balance.trans, na.rm = TRUE),
      median_balance = median(balance.trans, na.rm = TRUE),
      sd_balance = sd(balance.trans, na.rm = TRUE),
      min_gutschrift = min(total_gutschrift, na.rm = TRUE),
      max_gutschrift = max(total_gutschrift, na.rm = TRUE),
      mean_gutschrift = mean(total_gutschrift, na.rm = TRUE),
      median_gutschrift = median(total_gutschrift, na.rm = TRUE),
      sd_gutschrift = sd(total_gutschrift, na.rm = TRUE),
      min_belastung = min(total_belastung, na.rm = TRUE),
      max_belastung = max(total_belastung, na.rm = TRUE),
      mean_belastung = mean(total_belastung, na.rm = TRUE),
      median_belastung = median(total_belastung, na.rm = TRUE),
      sd_belastung = sd(total_belastung, na.rm = TRUE)
    )

  # add suffix
  colnames(df_aggregated)[-1] <- paste(colnames(df_aggregated[-1]), suffix_name, sep = "_")

  return(df_aggregated)
}



summarize_transactions(buyers_balance_umsatz)

summarize_transaction_window(buyers_balance_umsatz, n_last_months = 2, suffix_name = "last_3_month")
```


## 8.2 Statistische Kennzahlen für Vermögen und Umsätze
```{r}
# Buyers
statistic_buyers <- summarize_transactions(buyers_balance_umsatz)

statistic_buyers_last_3_month <- summarize_transaction_window(buyers_balance_umsatz, n_last_months = 2, suffix_name = "last_3_month")

statistic_buyers_last_6_month <- summarize_transaction_window(buyers_balance_umsatz, n_last_months = 5, suffix_name = "last_6_month")

statistic_buyers <- inner_join(statistic_buyers, statistic_buyers_last_3_month, by = "account_id.trans") %>% inner_join(statistic_buyers_last_6_month, by = "account_id.trans")

statistic_buyers


# Non-Buyers
statistic_non_buyers <- summarize_transactions(non_buyers_balance_umsatz)

statistic_non_buyers_last_3_month <- summarize_transaction_window(non_buyers_balance_umsatz, n_last_months = 2, suffix_name = "last_3_month")

statistic_non_buyers_last_6_month <- summarize_transaction_window(non_buyers_balance_umsatz, n_last_months = 5, suffix_name = "last_6_month")

statistic_non_buyers <- inner_join(statistic_non_buyers, statistic_non_buyers_last_3_month, by = "account_id.trans") %>% inner_join(statistic_non_buyers_last_6_month, by = "account_id.trans")

statistic_non_buyers
```


## 8.3 Export/Import der Aggregationen
Die Aggregation dauern lange und werden deshalb hier als Parquet File exportiert, um sie gegebenenfalls wieder einzulesen.
```{r}
write_parquet(statistic_buyers, "data_export/statistic_buyers.parquet")
write_parquet(statistic_non_buyers, "data_export/statistic_non_buyers.parquet")

statistic_buyers <- read_parquet("data_export/statistic_buyers.parquet")
statistic_non_buyers <- read_parquet("data_export/statistic_non_buyers.parquet")

statistic_buyers
statistic_non_buyers
```

## 9. Kombination von Event Informationen & Kreditkarten Käufer
Kombinieren event-bezogener Informationen von KreditkartenKäufern und Nicht-Käufern.

## 9.1 Kombination
Durch die Kombination aus den dem Dataframe Data und statistic_data erhalten wir ein grosses Dataframe welches 102 Features beinhaltet. 

```{r}
# concat statistic_buyers with statistic_non_buyers
statistic_data <- rbind(statistic_buyers, statistic_non_buyers)
statistic_data <- statistic_data %>% rename("account_id" = "account_id.trans")
statistic_data

# Imputieren NA values mit 0
statistic_data[is.na(statistic_data)] <- 0


full_data <- inner_join(statistic_data, data, by = "account_id")
full_data
```

# 10. Bereinigung
Bereinigen unnötiger Informationen (z.B. IDs) und Überprüfen der 
Struktur der Modellierungsdaten mittels explorativer Datenanalyse.

## 10.1 Daten Bereinigen

### 10.1.1 NAN Werte Überprüfen
```{r}
count_na_in_columns <- function(df) {
    sapply(df, function(x) sum(is.na(x)))
}

count_na_in_columns(full_data)
```

### 10.1.2 Spalte droppen
```{r}
drop_columns <- function(data, columns_to_drop) {
  data <- subset(data, select = -c(columns_to_drop))
  return(data)
}

full_data <- full_data %>% select(-c("date.loan", "type.card", "type.disp"))

# Grund fuer drop:
# account_id -> key
# date.loan -> VIele Na. wird durch andere loan Features ersetzt
# type.card -> viele Na, wird durch has_card.card ersetzt
# type.disp -> Ist Immer nur "OWNER" somit kein Informationsgehalt

```

### 10.1.3 Kategorische Spalten One Hot Encoden
```{r}
one_hot_encode <- function(data, categorical_columns) {
  # Loop through each categorical column and apply one-hot encoding
  for (col in categorical_columns) {
    # Perform one-hot encoding using the model.matrix() function
    encoded_cols <- model.matrix(~ . - 1, data = data[, col, drop = FALSE])
    
    # Add the encoded columns to the original DataFrame
    data <- cbind(data, encoded_cols)
    
    # Remove the original categorical column
    data <- data[, -which(names(data) == col), drop = FALSE]
  }
  
  return(data)
}

chr_columns <- full_data %>% select_if(is.character)
chr_col <- colnames(chr_columns)
chr_col

full_data_onehot <- one_hot_encode(full_data, chr_col)

full_data_onehot
```

### 10.1.4 Logische Spalten
```{r}
convert_logical_columns_to_binary <- function(data, logical_columns) {
  # Loop through each specified logical column and convert it
  for (col in logical_columns) {
    if (col %in% names(data) && is.logical(data[[col]])) {
      data[[col]] <- as.integer(data[[col]])
    } else {
      cat("Warning: Column", col, "is not a valid logical column or doesn't exist.\n")
    }
  }
  
  return(data)
}

logical_cols = c("has_card.card", "has_order.order", "has_loan.loan")
full_data_onehot <- convert_logical_columns_to_binary(full_data_onehot, logical_cols)
full_data_onehot
```

### 10.1.5 Target Spalte neu anordnen
```{r}
# Reorder Col, Target Col as last Column
full_data_onehot <- full_data_onehot %>% relocate(has_card.card, .after = last_col())
full_data_onehot

full_data <- full_data %>% relocate(has_card.card, .after = last_col())
full_data
```

### 10.1.6 Export Full Data
```{r}
write_parquet(full_data_onehot, "data_export/full_data_onehot.parquet")
full_data_onehot <- read_parquet("data_export/full_data_onehot.parquet")
full_data_onehot

write_parquet(full_data, "data_export/full_data.parquet")
full_data <- read_parquet("data_export/full_data.parquet")
full_data
```


### 10.1.7 Cache Leeren
```{r}
all_objects <- ls()
remove_objects <- setdiff(all_objects, c("full_data"))
rm(list = remove_objects)
print(ls())
```

## 10.2 Daten Explorieren

### 10.2.1 Anzahl Kreditkartenbesitzer
Anzahl Kreditkarten-Käufer und Nicht-Käufer mit kompletter 12 
Monate-Rollup Information (Barplot).
```{r fig.height=6}
# Count the number of has_card.card 
full_data %>% group_by(has_card.card) %>% summarize(count = n()) %>%  ggplot(aes(x = has_card.card, y = count)) +
  geom_bar(stat = "identity", fill = "blue") +
  geom_text(aes(label = count), vjust = -0.3, size = 4) +  # Add count values on top
  labs(
    title = "Kartenbesitzer",
    x = "Besitzt eine Karte",
    y = "Anzahl"
  )
```

### 10.2.2 Verteilung der Kaufzeitpunkte
Verteilung der Kaufzeitpunkte der Kreditkarten-Käufer bzw. Vergleichszeitpunkte der Nicht-Käufer (Densityplot).
```{r}
full_data %>% 
  select(issued.card, has_card.card) %>% 
  ggplot(aes(x = issued.card, fill = factor(has_card.card))) +
  geom_density(alpha = 0.25) + 
  labs(
    title = "Density Plot of Issued Cards",
    subtitle = "Grouped by has_card.card",
    x = "Dates",
    y = "Density"
  ) + 
  scale_color_manual(values = c("1" = "blue", "0" = "red"), name = "has_card")

```


# 11. Partitionierung
Partitionieren der Daten in Trainings-, Validierungs- und Testdaten.
Wir trennen die Daten gleichmässig aufgrund der Spalte has_card.card damit wir das gleiche Verhältnis von Kartenbesitzer und Nicht Kartenbesitzer im Trainings-, Validierungs-, und Testdaten haben.

## 11.1 Test Daten 10%
Wir entfernen 10% unserer Daten für den Testdatensatz und Exportieren diese als Parquet File. 

```{r}
# sample 10% of the data but keep the same ratio of has_card.card add seed for reproducibility
set.seed(123)
test_data <- full_data %>%
    group_by(has_card.card) %>%
    sample_frac(0.1) %>%
    ungroup()

test_data

# remove test data from full data
rest_data <- full_data %>%
    anti_join(test_data, by = "account_id")
rest_data
```

## 11.2 Train-, Validierungsdaten
Nun nehmen wir den restlichen Datensatz und trennen diese in Trainings und Validierungsdatensatz auf. Dabei nehmen wir ein Verhältnis von 80% Trainingsdaten und 20% Validierungsdaten, auf den restlichen Datensatz gerechnet. (Der Testdatensatz wird abgespeichert und in einem späteren Kapitel verwendet für die Model Performance auf unvorhergesehene Daten)

anti_join()
- anti_join() return all rows from x without a match in y.
```{r}
# Split the remaining data into train 80 % and validation data 20% make sure there are no data leakage between the three datasets
set.seed(123)
train_data <- rest_data %>%
    group_by(has_card.card) %>%
    sample_frac(0.8) %>%
    ungroup()

train_data

# remove train data from full data
val_data <- rest_data %>%
    anti_join(train_data, by = "account_id")
val_data
```

## 11.3 Data Leakage und Ratio Test Test
Data Leakage wird getestet durch semi_join()
- semi_join() return all rows from x with a match in y.
```{r}
# check if there is any data leakage by account_id between the three datasets

ratio <- function(data, column) {
    data %>%
        group_by({{ column }}) %>%
        summarise(n = n()) %>%
        mutate(ratio = n / sum(n))
}


ratio(rest_data)
ratio(test_data, has_card.card)
ratio(train_data, has_card.card)
ratio(val_data, has_card.card)

# create a function to check for data leakage
check_data_leakage <- function(data1, data2, column) {
    data1 %>%
        semi_join(data2, by = {{ column }}) %>% nrow()
}

# check for data leakage between train and validation data
check_data_leakage(train_data, val_data, "account_id")

# check for data leakage between train and test data
check_data_leakage(train_data, test_data, "account_id")

# check for data leakage between validation and test data
check_data_leakage(val_data, test_data, "account_id")

# check for data leakage between test and rest_data
check_data_leakage(test_data, rest_data, "account_id")

```


# 12. Setup (~ from scratch)
Bevor wir mit dem eigentlichen Modelieren starten definieren wir hier Funktionen die uns später unterstützen beim Evaluaieren. 

## 12.1 Model Evaluation
Die Funktion Model Evaluation benoetigt als Input das Model und test_date. Die Funktion macht predictions auf den test_data und iteriert durch unterschiedliche tresholds. Anschliessend berechnet es die Konfusionsmatrix und deren Metriken und gibt als Output ein Dataframe mit den Metriken, welches für die Evaluierung weiter verwendet werden kann. 
```{r}
model_evaluation <- function(model, val_data) {
    # Compute probabilities
    proba <- predict(model, val_data, type = "response")

    # Initialize dataframe with metrics
    metriken <- data.frame(
        threshold = numeric(),
        accuracy = numeric(),
        precision = numeric(),
        recall = numeric(),
        f1_score = numeric(),
        f0.5_score = numeric(),
        f2_score = numeric(),
        matthews = numeric(),
        TPR = numeric(), # True Positive Rate
        FPR = numeric() # False Positive Rate
    )

    # Iterate through different threshold values
    thresholds <- seq(0, 1, by = 0.1)

    for (threshold in thresholds) {
        predicted_classes <- ifelse(proba >= threshold, TRUE, FALSE)

        conf_matrix <- table(
            Predicted = as.factor(predicted_classes),
            Actual = as.factor(val_data$has_card.card)
        )
        # Initialize variables with default values
        tn <- 0
        tp <- 0
        fn <- 0
        fp <- 0
        
      # Check if the confusion matrix has the necessary dimensions
      if (nrow(conf_matrix) >= 2 && ncol(conf_matrix) >= 2) {
        tn <- conf_matrix[1, 1]
        tp <- conf_matrix[2, 2]
        fn <- conf_matrix[1, 2]
        fp <- conf_matrix[2, 1]
      }

        # Calculate accuracy
        accuracy <- (tp + tn) / (tn + tp + fn + fp)
        # Calculate precision
        precision <- tp / (tp + fp)
        # Calculate recall
        recall <- tp / (tp + fn)
        # Calculate f1 score
        f1_score <- 2 * precision * recall / (precision + recall)
        # Calculate f0.5 score
        f0.5_score <- (1 + 0.5^2) * precision * recall / ((0.5^2 * precision) + recall)
        # Calculate f2 score
        f2_score <- (1 + 2^2) * precision * recall / ((2^2 * precision) + recall)
        # Calculate Matthews correlation coefficient
        matthews <- (tp * tn - fp * fn) /
            sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))

        # Calculate true positive rate and false positive rate
        TPR <- tp / (tp + fn)
        FPR <- fp / (fp + tn)

        # Append metrics to the metrics dataframe
        metriken <- rbind(metriken, data.frame(
            threshold = threshold,
            accuracy = accuracy,
            precision = precision,
            recall = recall,
            f1_score = f1_score,
            f0.5_score = f0.5_score,
            f2_score = f2_score,
            matthews = matthews,
            TPR = TPR,
            FPR = FPR
        ))
    }
    evaluation_df <- metriken
    return(evaluation_df)
}
```

## 12.2 ROC Kurve 
Die plot_roc_curve nimmt als Input Parameter die evualtion_df, welches von der Funktion model_evaluation entsteht und visualisiert die ROC Kurve. 
```{r}
# plot ROC curve with input metriken
plot_roc_curve <- function(evaluation_df) {
    ggplot(evaluation_df, aes(x = FPR, y = TPR)) +
        geom_line() +
        coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) +
        geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
        labs(
            title = "ROC Curve",
            subtitle = "Receiver Operating Characteristic Curve",
            x = "False Positive Rate",
            y = "True Positive Rate"
        )
}
```

# 13. Baseline Modell (~ from scratch)
Erstellen eines Baseline Modelles mittels logistischer Regression und 
den Informationen “Alter”, “Geschlecht”, “Domizilregion”, 
“Vermögen” und “Umsatz” vor Kreditkartenkauf. 

## 13.1 Baseline Daten
```{r}
val_data %>% select(has_card.card, age.client, gender.client, region.district, mean_balance, mean_gutschrift, mean_belastung) %>% arrange(desc(has_card.card))
```

## 13.1 Baseline Modell erstellen
```{r}
base_line_model <- glm(has_card.card ~ age.client + gender.client + region.district + mean_balance + mean_gutschrift + mean_belastung, data = train_data, family = binomial)
```

## 13.2 Baseline Modell evaluieren
```{r}
baseline_evaluation <- model_evaluation(base_line_model, val_data)
baseline_evaluation

plot_roc_curve(baseline_evaluation)
```

## 13.3 Cross-Validierung k-fold = 10
Weil ich die Aufgabe nicht gelesen habe, dass wir Crossvalidieren sollten, mache ich auf den `rest_data` eine cross-validierung.

Die Funktion `create_kfold` erlaubt es mir eine definierte anzahl an folds den `rest_data` in gleichmässige Verhältnisse mittels `strata_col` zu splitten. Als Output erhalte ich eine Liste, indem jeweils der train durch und valdierungsdatensatz ist für jeden Fold. Die Daten die ich kriege werden für alle Modelle genutzt, damit die Reproduzierbarkeit sowie die Vergleichbarkeit gewährleistet ist.
```{r}
create_kfold <- function(rest_data, num_folds = 10, strata_col = has_card.card) {
  set.seed(19981218)
  cross_val_folds <- vfold_cv(rest_data, v = num_folds, strata = has_card.card)
  fold_datasets <- list()
  for (i in 1:nrow(cross_val_folds)) {
    train_data <- cross_val_folds$splits[[i]] %>% analysis()
    val_data <- cross_val_folds$splits[[i]] %>% assessment()
    fold_datasets[[i]] <- list(train_data = train_data, val_data = val_data)
  }
  return(fold_datasets)
}
kfolds_data <- create_kfold(rest_data)
```

## 13.4 Baseline Model mit k-fold=10
Hier in diesem Abschnitt erstelle ich aus den kfolds Daten das Baseline Modell und werte diese anschliessend mit der `model_evaluation` Funktion aus die im Abschnitt 12.1 erstellt wurde. 
```{r}
models <- list()
models_eval <- list()
for (i in 1:length(kfolds_data)) {
    train_data <- kfolds_data[[i]]$train_data
    val_data <- kfolds_data[[i]]$val_data
    base_line_model <- glm(has_card.card ~ age.client +
                             gender.client + region.district +
                             mean_balance + mean_gutschrift +
                             mean_belastung, data = train_data,
                           family = binomial)
    models[[i]] <- base_line_model
    models_eval[[i]] <- model_evaluation(base_line_model, val_data) 
    models_eval[[i]]$baseline_model_nr = i
}
```

## 13.5 Plot 10 Baseline Roc Cruve
Nun füge ich alle Ausgewerteten Baseline Modelle zusammen zu einem grossen Dataframe und kann den Datensatz später weiter gebrauchen für weitere Auswertungen. Vorweg Plotte ich die ROC Kurve und berechne für jeden Threshold den Mittelwert der Metriken sowie die Standardabweichung dazu. 
```{r}
all_baseline_results <- do.call(rbind, models_eval)

all_baseline_results %>% ggplot(aes(x = FPR, y = TPR, color = as.factor(baseline_model_nr))) + 
  geom_line() +
        coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) +
        geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
        labs(
            title = "ROC Curve",
            subtitle = "Receiver Operating Characteristic Curve",
            x = "False Positive Rate",
            y = "True Positive Rate"
        ) + 
  scale_color_discrete(name = "Kfold Baseline Models")

all_baseline_results %>%  group_by(threshold) %>%
  summarize(
    threshold = first(threshold),
    mean_accuracy = mean(accuracy, na.rm = TRUE),
    sd_accuracy = sd(accuracy, na.rm = TRUE),
    mean_precision = mean(precision, na.rm = TRUE),
    sd_precision = sd(precision, na.rm = TRUE),
    mean_recall = mean(recall, na.rm = TRUE),
    sd_recall = sd(recall, na.rm = TRUE),
    mean_f1_score = mean(f1_score, na.rm = TRUE),
    sd_f1_score = sd(f1_score, na.rm = TRUE),
    mean_f0.5_score = mean(f0.5_score, na.rm = TRUE),
    sd_f0.5_score = sd(f0.5_score, na.rm = TRUE),
    mean_f2_score = mean(f2_score, na.rm = TRUE),
    sd_f2_score = sd(f2_score, na.rm = TRUE),
    mean_matthews = mean(matthews, na.rm = TRUE),
    sd_matthews = sd(matthews, na.rm = TRUE),
    mean_TPR = mean(TPR, na.rm = TRUE),
    sd_TPR = sd(TPR, na.rm = TRUE),
    mean_FPR = mean(FPR, na.rm = TRUE),
    sd_FPR = sd(FPR, na.rm = TRUE)
  )

```






# 14 Model Setup
Nach der Kontaktstunde mit Dani wurde mir bewusst, dass die vorherhigen Kapitel 12 und 13 durch libraries ersetzt werden können. Tidymodels - das Stichwort oder besser gesagt die Erlösung. Nichts desto trotz wollte ich diese beiden vorherhigen Kapitel nicht löschen, sie sollen zeigen, dass ich mich mit der Materie befasst habe. In diesem Kapitel erstellen wir wieder unser Baseline Modell mit den bekannten Features die wir kennen und plotten die ROC Kurve, wie auch die Resultate von jedem Fold in einem Dataframe.

Wieder beginnen wir damit Funktionen zu schreiben, die uns nützlich sind in der Modellierung und Klassifizierung sowie Evaluierung auf unseren KFold Datensatz.

## 14.1 Funktion - Baseline Daten + kFolds
Die Funktion  ``create_baseline_data`` filtert uns die wichtigsten Features aus unserem grossen Datensatz. Diese Feature werden verwendet für unser Baseline Model. 

Auch erstellen wir direkt aus dem ganzen Datensatz die Daten Partitionierung bzw. unsere Kfolds. 

```{r}
create_baseline_data <- function(data) {
  baseline_data <- data %>%
    select(has_card.card, age.client, gender.client, region.district, mean_balance, mean_gutschrift, mean_belastung) %>%
    arrange(has_card.card)
  
  return(baseline_data)
}

baseline_train_data <- create_baseline_data(rest_data)
baseline_test_data <- create_baseline_data(test_data)

baseline_train_data
baseline_test_data

set.seed(123)
num_folds <- 10

data_partition_base <- createFolds(baseline_train_data$has_card.card, k = num_folds)
data_partition_full <- createFolds(rest_data$has_card.card, k = num_folds)

```

## 14.2 Funktion Baseline Model (Logistische Regression)

Die Funktion `train_and_evaluate_lr` ist ein glm Model, welches eine Binäre Klassifkation durchführt mit der Zielvariable `has_card.card`. Diese Baseline Funktion dient als Grundlage für weitere Model Funktionen, welche später hinzukommen.

```{r}
train_and_evaluate_lr <- function(train_index) {
  train_data <- baseline_train_data[-train_index, ]
  validation_data <- baseline_train_data[train_index, ]
  
  # Train the Logistic Regression Model
  lr_model <- glm(has_card.card ~ ., 
                  data = train_data, 
                  family = binomial())

  # Prediction on the Validation Data
  lr_validation_prediction <- predict(lr_model, newdata = validation_data, type = "response")
  
  # Create a data frame for prediction results
  lr_pred_validation <- data.frame(has_card_prob = lr_validation_prediction, 
                                   has_card_decision = as.factor(ifelse(lr_validation_prediction >= 0.5, 1, 0)),
                                   has_card.card = as.factor(ifelse(validation_data$has_card.card == "TRUE", 1, 0)))
  
  # Modify to return both AUC and ROC curve data
  roc_data <- roc(lr_pred_validation$has_card.card, lr_pred_validation$has_card_prob)
  auc <- auc(roc_data)
  
  # Calculate additional metrics
  confusion_matrix <- table(Predicted = lr_pred_validation$has_card_decision, 
                            Actual = lr_pred_validation$has_card.card)
  tp <- confusion_matrix[2, 2]
  tn <- confusion_matrix[1, 1]
  fp <- confusion_matrix[1, 2]
  fn <- confusion_matrix[2, 1]
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
  precision <- tp / (tp + fp)
  recall <- tp / (tp + fn)
  f1_score <- 2 * precision * recall / (precision + recall)
  f05_score <- (1 + 0.5^2) * precision * recall / (0.5^2 * precision + recall)
  f2_score <- (1 + 2^2) * precision * recall / (2^2 * precision + recall)
  mcc <- (tp * tn - fp * fn) / sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))

  return(list(auc = auc, 
              roc_curve = roc_data, 
              accuracy = accuracy, 
              precision = precision, 
              recall = recall,
              f1_score = f1_score,
              f05_score = f05_score,
              f2_score = f2_score,
              mcc = mcc))
}

```

## 14.3 Funktion Cross-Validation

Die Funktion `perform_cross_validation` führt auf jeden kfold die vorherhige implementierte `train_and_evaluation` Funktion aus und gibt dabei zwei wichtige Variabeln zurück, ein DataFrame welches alle Metriken enthalten und die ROC-Kurve, welche uns für die Auswertung sowie Visualisierung nützlich sein wird.

```{r}
perform_cross_validation <- function(data_partition, 
                                     train_eval_function) {
  roc_curves <- list()
  results_df <- data.frame(Fold = integer(), 
                           AUC = numeric(), 
                           Accuracy = numeric(), 
                           Precision = numeric(), 
                           Recall = numeric(),
                           f1_Score = numeric(),
                           f05_Score = numeric(),
                           f2_Score = numeric(),
                           mcc = numeric())

  # Perform cross-validation
  for (fold in 1:num_folds) {
    result <- train_eval_function(data_partition[[fold]])
    roc_curves[[fold]] <- result$roc_curve
    results_df <- rbind(results_df, data.frame(Fold = fold, 
                                               AUC = result$auc, 
                                               Accuracy = result$accuracy, 
                                               Precision = result$precision, 
                                               Recall = result$recall,
                                               f1_score = result$f1_score,
                                               f05_score = result$f05_score,
                                               f2_score = result$f2_score,
                                               mcc = result$mcc))
  }
  
  # describe dataframe into 5 statistical summaries
  results_df_stats <- summary(results_df)

  return(list(roc_curves = roc_curves, results_df = results_df, 
              results_df_stats = results_df_stats))
}


```


## 14.4 Funktion um Roc Kurve zu plotten

Die Funktion `plot_roc` nimmt von den output der vorherhigen Funktion `perform_cross_validation` und hat dabei zwei Parameter `roc_folds, roc_model`, welches uns erlaubt, entweder die Roc Kurven von allen Kfolds und/oder vom Model zu plotten.
Weiter gibt die Funktion ein Aggregiertes DataFrame zurück, damit wir später für den Model Vergleich, unterschiedliche Modelle miteinander visusalisieren können.

```{r}
plot_roc <- function(roc_curves, roc_folds = FALSE, roc_model = FALSE) {
  plot_data <- data.frame()
  
  num_folds <- length(roc_curves)
  
  for (fold in 1:num_folds) {
    fold_data <- data.frame(TPR = roc_curves[[fold]]$sensitivities,
                            FPR = 1 - roc_curves[[fold]]$specificities,
                            Fold = as.factor(fold))
    
    plot_data <- rbind(plot_data, fold_data)
  }
  
  # plot roc curve
  roc_plot <- ggplot(plot_data, aes(x = FPR, y = TPR, color = Fold)) +
    geom_line() +
    labs(title = "ROC Kurve für jeden Fold", x = "False Positive Rate", y = "True Positive Rate") +
    theme_minimal() +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray")
  
  if (roc_folds) {
    print(roc_plot)
  }
  
  aggregated_data <- plot_data %>%
    group_by(FPR) %>%
    summarise(
      MEAN_TPR = mean(TPR),
      SD_TPR = sd(TPR),
      Upper = MEAN_TPR + SD_TPR,
      Lower = MEAN_TPR - SD_TPR
    )
  
  # plot aggregated roc curve
  aggregated_plot <- aggregated_data %>%
    ggplot(aes(x = FPR, y = MEAN_TPR)) + 
    geom_line(color = "lightblue") + 
    geom_ribbon(aes(ymin = Lower, ymax = Upper), alpha = 0.2) +
    labs(title = "ROC Model Performance", x = "False Positive Rate", y = "True Positive Rate", subtitle = "Durchschnittliche Model Performance mit 1 Sigma") + 
    theme_minimal() +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray")
  
  if (roc_model) {
    print(aggregated_plot)
  }
  
  return(list(data = aggregated_data))
}
```


# 15 Baseline Model
Nun geht es darum, die Bausteine aus Kapitel 14 zusammenzufügen sowie zu testen, ob wir die gewünschten Resultate erhalten. Dies tun wir direkt, indem wir unser Baseline Model mit den wenigen Features erstellen. 
Bemerkung: Die Daten haben wir oben im Kapitel 14.1 schon vorbereitet.

## 15.1 Baseline Model 
```{r}
cv_results_base <- perform_cross_validation(data_partition_base, train_and_evaluate_lr)

roc_curves_base <- cv_results_base$roc_curves
results_df_base <- cv_results_base$results_df
results_df_base_stats <- cv_results_base$results_df_stats

results_df_base_stats
results_df_base

result_base <- plot_roc(roc_curves_base, roc_folds = TRUE, roc_model = TRUE)
result_base$data

```

## 15.2 Verbesserung des Baseline Modell
Systematisches Explorieren von Verbesserungsmöglichkeiten des 
Baseline Modelles durch Erweiterung erklärender Variablen, 
Verwendung anderer Algorithmen und Optimierung.
```{r}
cv_results_full <- perform_cross_validation(data_partition_full, train_and_evaluate_lr)

roc_curves_full <- cv_results_full$roc_curves
results_df_full <- cv_results_full$results_df
results_df_full_stats <- cv_results_full$results_df_stats

results_df_full_stats
results_df_full


result_full <- plot_roc(roc_curves_full, roc_folds = TRUE, roc_model = TRUE)
result_full$data

```

## 15.3 Andere Algorithmen - Random Forest Model

Zuerst schreiben wir die `train_and_evaluate` Funktion, diesmal statt mit der Logistische Regression, nutzen wir den Random Forest. Ansonsten belibt die Funktion in seiner Grundstruktur die selbe, hier hätte man durchaus code Wiederholungen verhindern können. Jedoch habe ich mich einfachhalts halber mal entschieden dies so zu lassen und zu einem späteren Zeitpunkt aufzuräumen, falls ich noch Zeit und Kapazität habe (18.12.23).

```{r}
# Create a function to train the Random Forest model and evaluate
train_and_evaluate_rfc <- function(train_index) {
  # Subset the training data for the current fold
  train_data <- baseline_train_data[-train_index, ]
  validation_data <- baseline_train_data[train_index, ]
  
  
  # Train the Random Forest Model
  rcf_model <- ranger(has_card.card ~ ., 
                      data = train_data, 
                      classification = TRUE, 
                      probability = TRUE)

  
  # Prediction on the Validation Data
  rfc_validation_prediction <- predict(rcf_model, data = validation_data)
  
  # Create a data frame for prediction results
  rfc_pred_validation <- data.frame(has_card_prob = rfc_validation_prediction$predictions[, 2], 
                                    # Use the second column for probability
                                    has_card_decision = as.factor(
                                      ifelse(
                                        rfc_validation_prediction$predictions[, 2] >= 0.5, 1, 0)
                                      ),
                                    has_card.card = as.factor(
                                      ifelse(
                                        validation_data$has_card.card == "TRUE", 1, 0)
                                      ))
  
  # Modify to return both AUC and ROC curve data
  roc_data <- roc(rfc_pred_validation$has_card.card, 
                  rfc_pred_validation$has_card_prob)
  
  auc <- auc(roc_data)
  
  # Calculate additional metrics
  confusion_matrix <- table(Predicted = rfc_pred_validation$has_card_decision, 
                            Actual = rfc_pred_validation$has_card.card)
  tp <- confusion_matrix[2, 2]
  tn <- confusion_matrix[1, 1]
  fp <- confusion_matrix[1, 2]
  fn <- confusion_matrix[2, 1]
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
  precision <- tp / (tp + fp)
  recall <- tp / (tp + fn)
  f1_score <- 2 * precision * recall / (precision + recall)
  f05_score <- (1 + 0.5^2) * precision * recall / (0.5^2 * precision + recall)
  f2_score <- (1 + 2^2) * precision * recall / (2^2 * precision + recall)
  mcc <- (tp * tn - fp * fn) / sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))

  return(list(auc = auc, 
              roc_curve = roc_data, 
              accuracy = accuracy, 
              precision = precision, 
              recall = recall,
              f1_score = f1_score,
              f05_score = f05_score,
              f2_score = f2_score,
              mcc = mcc))
}

```

```{r}
cv_results_rfc <- perform_cross_validation(data_partition_base, train_and_evaluate_rfc)

roc_curves_rfc <- cv_results_rfc$roc_curves
results_df_rfc <- cv_results_rfc$results_df
results_df_rfc_stats <- cv_results_rfc$results_df_stats

results_df_rfc_stats
results_df_rfc

result_rfc <- plot_roc(roc_curves_rfc, roc_folds = TRUE, roc_model = TRUE)
result_rfc$data
```


### 15.4 Andere Optimierungen


# 16. Vergleich der Modelle
Vergleichen der Kandidatenmodelle und identifizieren des bzgl. 
Performance “besten” Modelles mit ROC, AUC und Precision.

## 16.1 Modell Vergleich
```{r}
result_base$data
result_full$data
result_rfc$data
```


# 17. Top-N Kunden 
 Quantitatives Untersuchen der Unterschiede von Top-N Kunden Listen verschiedener Modelle
 
## 17.1 Top-N Kunden Vergleich von Modellen
```{r}

```

 
# 18. Wichtigkeit und Balancierung
Untersuchen der globalen Wichtigkeit der Einflussfaktoren des 
“besten” Modelles und Reduzieren des “besten” Modelles mittels 
Balancieren von globaler Wichtigkeit und Modellperformance.

## 18.1 Wichtigkeit
```{r}

```

## 18.2 Balancierung 
```{r}

```


# 18. Beschreibung 
Beschreiben des Mehrwerts des “finalen” Modelles in der Praxis.
```{r}

```

# Lieferobjekte
Die minimalen Lieferobjekte im Notebook umfassen:

## Anzahl Kreditkarten-Käufer und Nicht-Käufer mit kompletter 12 
Monate-Rollup Information (Barplot).

## Verteilung der Kaufzeitpunkte der Kreditkarten-Käufer bzw. Vergleichszeitpunkte der Nicht-Käufer (Densityplot).

## Beschreibung der konstruierten, eigenen Variablen.

## Beschreibung von Baseline und mind. 3 Kandidaten-Modellen.

## Performance-Vergleich von Baseline und mind. 3 KandidatenModellen für 10-fache Kreuzvalidierung (Metrik: ROC Kurve und AuC). PRÄDIKTIVE MODELLE Mini-Challenge Lieferobjekte

## Performance-Vergleich von Baseline und mind. 3 KandidatenModellen für Testdaten (Metriken: Konfusionsmatrix, Accuracy,Kohen’s Kappa, Matthew’s Korrelation, Precision, Recall und F1 als Tabelle, ROC Kurve und AuC als Abbildung).

## Globale Wichtigkeit der Einflussfaktoren von Baseline und mind. 3. Kandidaten-Modellen (Variable Importance Plot).

## Quantifizierung der Unterschiede von Top-5%, Top-10% KundenListen für Baseline und mind. 3 Kandidaten-Modellen (eigene Ideen).

## Modellgüte (Lift Kurve) und Einfluss zentraler Variablen des finalen Modelles (eigene Ideen, um Non-Data Scientists zu überzeugen).
