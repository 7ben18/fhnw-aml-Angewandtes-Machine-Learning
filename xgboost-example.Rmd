---
title: "Xgboost example"
output: html_notebook
---
# 1. Install and Load Required Libraries

```{r}

library(xgboost)
library(caret)
library(pROC)
library(Metrics)

```

# 2. Prepare the Data

```{r}
data(iris)
set.seed(123)

# Convert to a binary classification problem
iris$Species <- as.numeric(iris$Species == "virginica")

# Split the data into features and labels
X <- as.matrix(iris[, -5])
y <- iris$Species

```

# 3. Define the Cross-Validation Method

```{r}
k <- 10
folds <- createFolds(y, k = k, list = TRUE, returnTrain = FALSE)

```

```{r}
f1_score <- function(true_labels, predicted_labels) {
  precision <- sum(true_labels == 1 & predicted_labels == 1) / sum(predicted_labels == 1)
  recall <- sum(true_labels == 1 & predicted_labels == 1) / sum(true_labels == 1)
  f1 <- 2 * precision * recall / (precision + recall)
  return(f1)
}

```


# 4. Train the Model and Evaluate

```{r}
results <- data.frame(Fold = integer(0), Accuracy = numeric(0), F1 = numeric(0))
roc_list <- list()

for(i in 1:k){
  # Splitting data into training and testing sets
  test_indices <- folds[[i]]
  train_indices <- setdiff(1:length(y), test_indices)
  
  dtrain <- xgb.DMatrix(data = X[train_indices, ], label = y[train_indices])
  dtest <- xgb.DMatrix(data = X[test_indices, ], label = y[test_indices])
  
  # Parameters for binary classification
  params <- list(
    objective = "binary:logistic",
    eval_metric = "logloss",
    eta = 0.3,
    max_depth = 6
  )
  
  # Train the model
  bst <- xgb.train(params = params, data = dtrain, nrounds = 100)
  
  # Predictions
  pred_prob <- predict(bst, dtest)
  pred_label <- ifelse(pred_prob > 0.3, 1, 0)
  
  # Evaluate
  acc <- accuracy(y[test_indices], pred_label)
  f1 <- f1_score(y[test_indices], pred_label)
  
  # Store results
  results <- rbind(results, data.frame(Fold = i, Accuracy = acc, F1 = f1))
  
  # ROC
  roc_list[[i]] <- roc(response = y[test_indices], predictor = pred_prob)
}

# Print the results
print(results)

```
5. Plot the ROC Curve


```{r}
plot(roc_list[[1]], col = "#1c61b6", main = "ROC Curves for each fold")
for(i in 2:k){
  plot(roc_list[[i]], add = TRUE, col = i)
}
legend("bottomright", legend = paste("Fold", 1:k), fill = 1:k)

```




```{r}
# Define colors
colors <- rainbow(k)

# Plot the ROC Curve for each fold
plot(roc_list[[1]], col = colors[1], main = "ROC Curves for each fold", xlim = c(0, 1), ylim = c(0, 1))
for(i in 2:k){
  plot(roc_list[[i]], add = TRUE, col = colors[i])
}

# Add a legend
legend("bottomright", legend = paste("Fold", 1:k), col = colors, lwd = 2)

```

```{r}
# Define transparent colors
colors <- rgb(red = 1:k/k, green = rev(1:k)/k, blue = (1:k)^2/k^2, alpha = 0.5)

# Plot the ROC Curve for each fold with transparent colors
plot(roc_list[[1]], col = colors[1], main = "ROC Curves for each fold", xlim = c(0, 1), ylim = c(0, 1))
for(i in 2:k){
  plot(roc_list[[i]], add = TRUE, col = colors[i])
}

# Add a legend
legend("bottomright", legend = paste("Fold", 1:k), col = colors, lwd = 2)

```

-------------------------------------------------------------------------




```{r}
library(xgboost)
library(caret)
library(pROC)
library(mlbench)

```


```{r}
# Load the Sonar dataset
data(Sonar)

# Inspect the data structure
str(Sonar)

# Convert factor to numeric binary classification (M:1, R:0)
Sonar$Class <- as.numeric(Sonar$Class == "M")

# Prepare the data
set.seed(123)
X <- as.matrix(Sonar[, -ncol(Sonar)])  # all columns except the last one
y <- Sonar$Class

```




```{r}
k <- 10
folds <- createFolds(y, k = k, list = TRUE, returnTrain = FALSE)

```


```{r}
results <- data.frame(Fold = integer(0), Accuracy = numeric(0), F1 = numeric(0))
roc_list <- list()

for(i in 1:k){
  # Splitting data into training and testing sets
  test_indices <- folds[[i]]
  train_indices <- setdiff(1:length(y), test_indices)
  
  dtrain <- xgb.DMatrix(data = X[train_indices, ], label = y[train_indices])
  dtest <- xgb.DMatrix(data = X[test_indices, ], label = y[test_indices])
  
  # Parameters for binary classification
  params <- list(
    objective = "binary:logistic",
    eval_metric = "logloss",
    eta = 0.3,
    max_depth = 6
  )
  
  # Train the model
  bst <- xgb.train(params = params, data = dtrain, nrounds = 100)
  
  # Predictions
  pred_prob <- predict(bst, dtest)
  pred_label <- ifelse(pred_prob > 0.5, 1, 0)
  
  # Evaluate
  acc <- sum(y[test_indices] == pred_label) / length(pred_label)
  f1 <- f1_score(y[test_indices], pred_label)  # Use the previously defined f1_score function
  
  # Store results
  results <- rbind(results, data.frame(Fold = i, Accuracy = acc, F1 = f1))
  
  # ROC
  roc_list[[i]] <- roc(response = y[test_indices], predictor = pred_prob)
}

# Print the results
print(results)

```


```{r}
colors <- rainbow(k)
plot(roc_list[[1]], col = colors[1], main = "ROC Curves for each fold", xlim = c(0, 1), ylim = c(0, 1))
for(i in 2:k){
  plot(roc_list[[i]], add = TRUE, col = colors[i])
}
legend("bottomright", legend = paste("Fold", 1:k), col = colors, lwd = 2)

```


```{r}
plot(roc_list[[1]], col = "#1c61b6", main = "ROC Curves for each fold")
for(i in 2:k){
  plot(roc_list[[i]], add = TRUE, col = i)
}
legend("bottomright", legend = paste("Fold", 1:k), fill = 1:k)
```





